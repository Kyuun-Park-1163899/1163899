{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d1237e0",
   "metadata": {},
   "source": [
    "# IT Incident SLA Comliance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c9fb2d",
   "metadata": {},
   "source": [
    "Service Level Agreement (SLA) compliance represents a critical performance metric in IT service management, directly impacting customer satisfaction, operational efficiency, and business continuity. Organizations invest substantial resources in incident management processes, yet many struggle with inconsistent SLA performance across different incident types, time periods, and organizational units. This comprehensive analysis examines IT incident data to identify systematic patterns affecting SLA compliance rates. Unlike traditional approaches that focus solely on incident closure times, this study investigates the complex interplay between incident classification systems (priority, impact, urgency), organizational factors (assignment groups, categories), process adherence variables (knowledge requirements, priority confirmation), and temporal patterns that influence service delivery outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4568f286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import chi2_contingency, pearsonr, ttest_ind\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_selection import mutual_info_classif, SelectKBest, chi2\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from scipy import stats\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07182767",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd6befe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "# Note: The '?' character is used to represent missing values in this dataset\n",
    "df = pd.read_csv('../data/incident_event_log_dataset.csv', na_values=['?'])\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Number of unique incidents: {df['number'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3636ca42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic dataset information for initial understanding\n",
    "print(\"\\nFirst 3 rows of the dataset:\")\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12db630c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display data types\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a9605c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic data information\n",
    "print(f\"\\nBasic dataset information:\")\n",
    "print(f\"- Total records: {len(df):,}\")\n",
    "print(f\"- Number of columns: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94740c59",
   "metadata": {},
   "source": [
    "## 2. Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ada251e",
   "metadata": {},
   "source": [
    "### (1) Data Cleansing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167a212c",
   "metadata": {},
   "source": [
    "Accurate time data is essential for SLA analysis precision. This analysis prioritizes processing \n",
    "date fields that form the basis for closed time calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff34195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date columns to datetime format for temporal analysis\n",
    "# These fields are critical for calculating closed time and temporal patterns\n",
    "date_columns = [\n",
    "    'opened_at', 'resolved_at', 'closed_at', 'sys_created_at', 'sys_updated_at'\n",
    "]\n",
    "\n",
    "for col in date_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "        print(f\"- {col}: converted to datetime\")\n",
    "    else:\n",
    "        print(f\"- {col}: column not found in dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0627fa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine incident states to understand data completeness\n",
    "print(f\"\\nIncident states in the dataset:\")\n",
    "if 'incident_state' in df.columns:\n",
    "    print(df['incident_state'].value_counts())\n",
    "else:\n",
    "    print(\"Warning: 'incident_state' column not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4249148f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for closed incidents only - we can only calculate closed time for completed incidents\n",
    "# This is essential for SLA compliance analysis as ongoing incidents don't have closed times\n",
    "closed_incidents = df[(df['incident_state'].isin(['Resolved', 'Closed']))\n",
    "                      &  # Must be completed\n",
    "                      (df['closed_at'].notna())  # Must have closed timestamp\n",
    "                      &\n",
    "                      (df['opened_at'].notna())  # Must have opening timestamp\n",
    "                      ].copy()\n",
    "\n",
    "print(f\"\\nFiltered to {len(closed_incidents):,} closed incidents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c894cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate closed time in hours - primary performance metric\n",
    "closed_incidents['closed_time_hours'] = (\n",
    "    closed_incidents['closed_at'] -\n",
    "    closed_incidents['opened_at']).dt.total_seconds() / 3600\n",
    "print(f\"Calculated closed time for all closed incidents\")\n",
    "\n",
    "# Remove data quality issues - negative closed times are logically impossible\n",
    "before_cleaning = len(closed_incidents)\n",
    "closed_incidents = closed_incidents[closed_incidents['closed_time_hours'] >= 0]\n",
    "after_cleaning = len(closed_incidents)\n",
    "print(f\"Removed {before_cleaning - after_cleaning} incidents with negative closed time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccedbdca",
   "metadata": {},
   "source": [
    "### (2) Handling Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee97244",
   "metadata": {},
   "source": [
    "- In IT incident data, missing values may represent business process characteristics \n",
    "rather than simple omissions.\n",
    "- The strategy adopted here preserves meaning over indiscriminate removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5741b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze missing data patterns to inform strategy\n",
    "print(\"Missing data analysis:\")\n",
    "missing_data = closed_incidents.isnull().sum()\n",
    "missing_percentage = (missing_data / len(closed_incidents)) * 100\n",
    "\n",
    "# Identify columns with significant missing data\n",
    "significant_missing = missing_percentage[missing_percentage > 5]\n",
    "if len(significant_missing) > 0:\n",
    "    print(\"Columns with >5% missing data:\")\n",
    "    for col, pct in significant_missing.items():\n",
    "        print(f\"- {col}: {missing_data[col]:,} missing ({pct:.1f}%)\")\n",
    "else:\n",
    "    print(\"No columns with significant missing data (>5%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d093a689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategic missing data handling approach\n",
    "# Only remove columns that are completely unusable (>95% missing)\n",
    "extreme_missing_count = (missing_percentage > 95).sum()\n",
    "columns_to_exclude = missing_percentage[missing_percentage > 95].index.tolist()\n",
    "\n",
    "print(f\"\\nMissing Data Strategy:\")\n",
    "print(f\"- Excluding {extreme_missing_count} columns with >95% missing data\")\n",
    "if columns_to_exclude:\n",
    "    print(f\"- Columns excluded: {columns_to_exclude}\")\n",
    "    closed_incidents = closed_incidents.drop(columns=columns_to_exclude)\n",
    "    print(f\"- Removed {len(columns_to_exclude)} unusable columns from dataset\")\n",
    "else:\n",
    "    print(\"- No columns found with >95% missing data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc82962e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Data Column Summary\n",
    "print(\"Columns with missing data:\")\n",
    "final_missing = closed_incidents.isnull().sum()\n",
    "for col, count in final_missing[final_missing > 0].items():\n",
    "    pct = (count / len(closed_incidents)) * 100\n",
    "    print(f\"- {col}: {count:,} missing ({pct:.1f}%)\")\n",
    "\n",
    "complete_cols = (final_missing == 0).sum()\n",
    "print(f\"\\nColumns with no missing data: {complete_cols}\")\n",
    "print(f\"Total columns: {len(closed_incidents.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189bf72e",
   "metadata": {},
   "source": [
    "### (3) Handling Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f94c133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse closed time distribution to identify outliers\n",
    "# Closed time is our key performance metric and outliers can significantly impact analysis\n",
    "closed_times = closed_incidents['closed_time_hours'].copy()\n",
    "\n",
    "print(f\"Closed time distribution analysis:\")\n",
    "print(f\"- Mean: {closed_times.mean():.2f} hours\")\n",
    "print(f\"- Median: {closed_times.median():.2f} hours\")\n",
    "print(f\"- Standard deviation: {closed_times.std():.2f} hours\")\n",
    "print(f\"- Min: {closed_times.min():.2f} hours\")\n",
    "print(f\"- Max: {closed_times.max():.2f} hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64852ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate skewness to understand distribution shape\n",
    "skewness = closed_times.skew()\n",
    "print(\n",
    "    f\"- Skewness: {skewness:.2f} ({'Highly right-skewed' if skewness > 2 else 'Moderately right-skewed' if skewness > 1 else 'Approximately normal'})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b9fda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection and extreme case identification\n",
    "Q1 = closed_incidents['closed_time_hours'].quantile(0.25)\n",
    "Q3 = closed_incidents['closed_time_hours'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "iqr_outliers = closed_incidents['closed_time_hours'] > (Q3 + 1.5 * IQR)\n",
    "\n",
    "max_closed = closed_incidents['closed_time_hours'].max()\n",
    "print(\"Outlier Analysis:\")\n",
    "print(\n",
    "    f\"Outliers detected: {iqr_outliers.sum():,} incidents ({iqr_outliers.mean():.1%})\"\n",
    ")\n",
    "print(\n",
    "    f\"CRITICAL: Maximum closed time = {max_closed:.0f} hours ({max_closed/24:.0f} days)\"\n",
    ")\n",
    "print(\n",
    "    f\"This represents a {max_closed/24:.0f}-day system failure that must be analyzed\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f680f8b3",
   "metadata": {},
   "source": [
    "**Retain outliers for complete SLA analysis**\n",
    "- Outliers are retained in the dataset for complete SLA compliance analysis\n",
    "- Extreme closed times represent real business scenarios that must be included in SLA performance evaluation, even if they skew statistical measures\n",
    "- These cases may reveal critical process breakdowns requiring attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62204c43",
   "metadata": {},
   "source": [
    "### (4) Final Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922de70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final analysis dataset with unique incidents only\n",
    "if 'closed_at' in closed_incidents.columns:\n",
    "    latest_incidents = closed_incidents.sort_values('closed_at').groupby(\n",
    "        'number').tail(1).copy()\n",
    "    print(\"Using latest closed record for each incident\")\n",
    "else:\n",
    "    latest_incidents = closed_incidents.drop_duplicates(subset=['number'],\n",
    "                                                          keep='last').copy()\n",
    "    print(\"Warning: closed_at not available, using simple deduplication\")\n",
    "\n",
    "\n",
    "# Deduplication verification\n",
    "duplicates_removed = len(closed_incidents) - len(latest_incidents)\n",
    "print(f\"\\nDeduplication summary:\")\n",
    "print(f\"- Before: {len(closed_incidents):,} records\")\n",
    "print(f\"- After: {len(latest_incidents):,} unique incidents\")\n",
    "print(f\"- Duplicates removed: {duplicates_removed:,}\")\n",
    "\n",
    "if duplicates_removed > 0:\n",
    "    duplicate_rate = duplicates_removed / len(closed_incidents)\n",
    "    print(f\"- Duplicate rate: {duplicate_rate:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2de36b",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319df67c",
   "metadata": {},
   "source": [
    "#### (1) SLA Performance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6f018c",
   "metadata": {},
   "source": [
    "This section analyzes the existing made_sla field to understand current SLA compliance patterns and performance characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a401680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if made_sla field exists\n",
    "if 'made_sla' not in latest_incidents.columns:\n",
    "    print(\"ERROR: made_sla field not found in dataset!\")\n",
    "    print(\"Available columns:\", list(latest_incidents.columns))\n",
    "else:\n",
    "    # Convert made_sla to binary format (TRUE/FALSE -> 1/0)\n",
    "    latest_incidents['sla_met'] = latest_incidents['made_sla'].map({\n",
    "        True: 1,\n",
    "        'TRUE': 1,\n",
    "        't': 1,\n",
    "        'T': 1,\n",
    "        False: 0,\n",
    "        'FALSE': 0,\n",
    "        'f': 0,\n",
    "        'F': 0\n",
    "    })\n",
    "    # SLA Target Variable Visualization\n",
    "    plt.figure(figsize=(24, 15))\n",
    "\n",
    "    # 1. SLA Compliance Distribution\n",
    "    plt.subplot(3, 4, 1)\n",
    "    compliance_counts = latest_incidents['sla_met'].value_counts().sort_index()\n",
    "    colors = ['red', 'green']\n",
    "    labels = ['SLA Breach', 'SLA Met']\n",
    "    plt.pie(compliance_counts.values,\n",
    "            labels=labels,\n",
    "            autopct='%1.1f%%',\n",
    "            colors=colors,\n",
    "            startangle=90)\n",
    "    plt.title('Overall SLA Compliance Distribution')\n",
    "\n",
    "    # 2. SLA Compliance by Priority\n",
    "    plt.subplot(3, 4, 2)\n",
    "    if 'priority' in latest_incidents.columns:\n",
    "        try:\n",
    "            priority_sla = pd.crosstab(latest_incidents['priority'],\n",
    "                                       latest_incidents['sla_met'],\n",
    "                                       normalize='index')\n",
    "            priority_sla.plot(kind='bar',\n",
    "                              stacked=True,\n",
    "                              color=['red', 'green'],\n",
    "                              alpha=0.8,\n",
    "                              ax=plt.gca())\n",
    "            plt.title('SLA Compliance by Priority')\n",
    "            plt.ylabel('Proportion')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.legend(['Breach', 'Met'])\n",
    "\n",
    "            # Add percentage labels on bars\n",
    "            for i, (priority, values) in enumerate(priority_sla.iterrows()):\n",
    "                breach_rate = values[0] if 0 in values.index else 0\n",
    "                compliance_rate = values[1] if 1 in values.index else 0\n",
    "\n",
    "                plt.text(i,\n",
    "                         breach_rate / 2,\n",
    "                         f'{breach_rate:.1%}',\n",
    "                         ha='center',\n",
    "                         va='center',\n",
    "                         fontweight='bold',\n",
    "                         color='white')\n",
    "                plt.text(i,\n",
    "                         breach_rate + compliance_rate / 2,\n",
    "                         f'{compliance_rate:.1%}',\n",
    "                         ha='center',\n",
    "                         va='center',\n",
    "                         fontweight='bold',\n",
    "                         color='black')\n",
    "\n",
    "        except Exception as e:\n",
    "            plt.text(0.5,\n",
    "                     0.5,\n",
    "                     f'Error: {str(e)}',\n",
    "                     transform=plt.gca().transAxes)\n",
    "            plt.title('SLA Compliance by Priority (Error)')\n",
    "    else:\n",
    "        plt.text(0.5,\n",
    "                 0.5,\n",
    "                 'Priority column not available',\n",
    "                 transform=plt.gca().transAxes)\n",
    "        plt.title('SLA Compliance by Priority (N/A)')\n",
    "\n",
    "    # 3. Closed Time Distribution by SLA Status\n",
    "    plt.subplot(3, 4, 3)\n",
    "    sla_met_times = latest_incidents[latest_incidents['sla_met'] ==\n",
    "                                     1]['closed_time_hours']\n",
    "    sla_breach_times = latest_incidents[latest_incidents['sla_met'] ==\n",
    "                                        0]['closed_time_hours']\n",
    "\n",
    "    if len(sla_met_times) > 0 and len(sla_breach_times) > 0:\n",
    "        # Use full data but limit X-axis for visual clarity\n",
    "        plt.hist(sla_met_times,\n",
    "                 bins=50,\n",
    "                 alpha=0.7,\n",
    "                 color='green',\n",
    "                 label='SLA Met',\n",
    "                 density=False)\n",
    "        plt.hist(sla_breach_times,\n",
    "                 bins=50,\n",
    "                 alpha=0.7,\n",
    "                 color='red',\n",
    "                 label='SLA Breach',\n",
    "                 density=False)\n",
    "        plt.xlabel('Closed Time (Hours)')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Closed Time Distribution by SLA Status')\n",
    "        plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Detailed Analysis\n",
    "    print(f\"\\n\" + \"=\" * 60)\n",
    "    print(f\"DETAILED ANALYSIS\")\n",
    "    print(f\"=\" * 60)\n",
    "\n",
    "    # Calculate basic SLA metrics for overview\n",
    "    overall_compliance = latest_incidents['sla_met'].mean()\n",
    "    total_incidents = len(latest_incidents)\n",
    "    breached_incidents = (latest_incidents['sla_met'] == 0).sum()\n",
    "\n",
    "    print(f\"\\nOverall SLA Performance Summary:\")\n",
    "    print(f\"- Total incidents analyzed: {total_incidents:,}\")\n",
    "    print(f\"- Overall compliance rate: {overall_compliance:.1%}\")\n",
    "    print(\n",
    "        f\"- SLA breaches: {breached_incidents:,} ({(1-overall_compliance):.1%})\"\n",
    "    )\n",
    "    print(\n",
    "        f\"- SLA compliant incidents: {total_incidents - breached_incidents:,} ({overall_compliance:.1%})\"\n",
    "    )\n",
    "\n",
    "    # Priority-specific metrics\n",
    "    if 'priority' in latest_incidents.columns:\n",
    "        print(f\"\\nSLA Compliance by Priority:\")\n",
    "        priority_stats = latest_incidents.groupby('priority').agg({\n",
    "            'sla_met': ['count', 'sum', 'mean'],\n",
    "            'closed_time_hours': ['mean', 'median']\n",
    "        }).round(3)\n",
    "        priority_stats.columns = [\n",
    "            'total_incidents', 'sla_met_count', 'compliance_rate',\n",
    "            'avg_closed', 'median_closed'\n",
    "        ]\n",
    "\n",
    "        for priority in priority_stats.index:\n",
    "            stats = priority_stats.loc[priority]\n",
    "            breach_count = stats['total_incidents'] - stats['sla_met_count']\n",
    "            breach_rate = 1 - stats['compliance_rate']\n",
    "            print(\n",
    "                f\"- {priority:15}: {stats['compliance_rate']:6.1%} compliance ({int(stats['sla_met_count']):,}/{int(stats['total_incidents']):,})\"\n",
    "            )\n",
    "            print(\n",
    "                f\"{'':17} Avg closed: {stats['avg_closed']:6.1f}h, Median: {stats['median_closed']:6.1f}h\"\n",
    "            )\n",
    "\n",
    "    # Closed time analysis by SLA status\n",
    "    if len(sla_met_times) > 0 and len(sla_breach_times) > 0:\n",
    "        print(f\"\\nClosed Time Analysis:\")\n",
    "        print(f\"- SLA Met incidents:\")\n",
    "        print(\n",
    "            f\"  * Average: {sla_met_times.mean():.1f} hours ({sla_met_times.mean()/24:.1f} days)\"\n",
    "        )\n",
    "        print(\n",
    "            f\"  * Median: {sla_met_times.median():.1f} hours ({sla_met_times.median()/24:.1f} days)\"\n",
    "        )\n",
    "        print(\n",
    "            f\"  * Max: {sla_met_times.max():.1f} hours ({sla_met_times.max()/24:.1f} days)\"\n",
    "        )\n",
    "\n",
    "        print(f\"- SLA Breach incidents:\")\n",
    "        print(\n",
    "            f\"  * Average: {sla_breach_times.mean():.1f} hours ({sla_breach_times.mean()/24:.1f} days)\"\n",
    "        )\n",
    "        print(\n",
    "            f\"  * Median: {sla_breach_times.median():.1f} hours ({sla_breach_times.median()/24:.1f} days)\"\n",
    "        )\n",
    "        print(\n",
    "            f\"  * Max: {sla_breach_times.max():.1f} hours ({sla_breach_times.max()/24:.1f} days)\"\n",
    "        )\n",
    "\n",
    "        print(f\"- Performance Gap:\")\n",
    "        avg_gap = sla_breach_times.mean() - sla_met_times.mean()\n",
    "        median_gap = sla_breach_times.median() - sla_met_times.median()\n",
    "        print(\n",
    "            f\"  * Average closed time gap: {avg_gap:.1f} hours ({avg_gap/24:.1f} days)\"\n",
    "        )\n",
    "        print(\n",
    "            f\"  * Median closed time gap: {median_gap:.1f} hours ({median_gap/24:.1f} days)\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036ad4b7",
   "metadata": {},
   "source": [
    "**Key Performance Insights**\n",
    "- Business SLA Framework: Analysis uses organization-defined SLA criteria from the made_sla field, providing direct measurement of actual business performance against established targets.\n",
    "- Priority Performance Paradox: Critical incidents achieve only 53.3% compliance compared to 95.5% for Low priority incidents, creating a 42.2% performance gap that suggests either resource constraints for complex issues or unrealistic SLA targets for high-priority incidents.\n",
    "- Limited Performance Gap: SLA breaches result in 89.6 hour (3.7 day) average delay compared to compliant incidents, though median gap is larger at 388.7 hours (16.2 days), indicating that while most breaches are modest, some cases experience significant delays.\n",
    "- Extended Closure Timeframes: Both SLA-met and breached incidents show extended closed times averaging 126-130 days, suggesting this dataset may represent complex projects or change requests rather than typical IT incidents, with maximum cases extending beyond 500 days."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca40eb8",
   "metadata": {},
   "source": [
    "#### (2) Priority/Impact/Urgency Classification Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb1e93c",
   "metadata": {},
   "source": [
    "This analysis quantitatively evaluates how incident classification systems (Priority, Impact, Urgency) affect SLA performance outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573d9d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classification variables for analysis\n",
    "classification_vars = ['priority', 'impact', 'urgency']\n",
    "available_classification = [\n",
    "    var for var in classification_vars if var in latest_incidents.columns\n",
    "]\n",
    "\n",
    "print(f\"Available classification variables: {available_classification}\")\n",
    "print(f\"Total incidents for analysis: {len(latest_incidents):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955fe034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing - safe numeric extraction and validation\n",
    "def safe_extract_numeric(series, column_name):\n",
    "    \"\"\"Safely extract and validate numeric codes\"\"\"\n",
    "    # Extract numbers using regex\n",
    "    numeric_series = series.str.extract('(\\d+)', expand=False)\n",
    "\n",
    "    # Attempt numeric conversion\n",
    "    try:\n",
    "        numeric_series = pd.to_numeric(numeric_series, errors='coerce')\n",
    "\n",
    "        # Check extraction success rate\n",
    "        success_rate = numeric_series.notna().mean()\n",
    "        print(f\"- {column_name}: {success_rate:.1%} extraction success\")\n",
    "\n",
    "        if success_rate < 0.8:  # Warning if less than 80%\n",
    "            print(\n",
    "                f\"  Warning: {column_name} extraction rate is low. Check data format.\"\n",
    "            )\n",
    "\n",
    "        return numeric_series\n",
    "    except Exception as e:\n",
    "        print(f\"- {column_name} numeric extraction failed: {e}\")\n",
    "        return pd.Series([np.nan] * len(series))\n",
    "\n",
    "\n",
    "# Extract numeric codes for each classification variable\n",
    "for var in available_classification:\n",
    "    if var in latest_incidents.columns:\n",
    "        latest_incidents[f'{var}_num'] = safe_extract_numeric(\n",
    "            latest_incidents[var], var)\n",
    "\n",
    "\n",
    "# Priority pattern analysis (actual data patterns instead of complex rules)\n",
    "def analyze_priority_patterns():\n",
    "    \"\"\"Analyze priority assignment patterns from actual data\"\"\"\n",
    "    if all(col in latest_incidents.columns\n",
    "           for col in ['impact_num', 'urgency_num', 'priority_num']):\n",
    "\n",
    "        # Generate actual priority matrix\n",
    "        try:\n",
    "            priority_matrix = latest_incidents.pivot_table(\n",
    "                values='priority_num',\n",
    "                index='impact_num',\n",
    "                columns='urgency_num',\n",
    "                aggfunc=lambda x: x.mode().iloc[0]\n",
    "                if len(x.mode()) > 0 else np.nan  # Most frequent value\n",
    "            )\n",
    "\n",
    "            print(\"\\nActual Priority Assignment Patterns (Impact vs Urgency):\")\n",
    "            print(priority_matrix)\n",
    "        except Exception as e:\n",
    "            print(f\"Priority matrix generation failed: {e}\")\n",
    "            # Alternative: use simple crosstab\n",
    "            priority_matrix = pd.crosstab(latest_incidents['impact_num'],\n",
    "                                          latest_incidents['urgency_num'],\n",
    "                                          latest_incidents['priority_num'],\n",
    "                                          aggfunc='mean')\n",
    "            print(\"\\nAlternative: Average Priority by Impact-Urgency:\")\n",
    "            print(priority_matrix)\n",
    "\n",
    "        # Consistency analysis\n",
    "        consistency_check = latest_incidents.groupby(\n",
    "            ['impact_num', 'urgency_num']).agg({\n",
    "                'priority_num': ['count', 'nunique', 'std']\n",
    "            }).round(2)\n",
    "\n",
    "        inconsistent_combinations = consistency_check[consistency_check[(\n",
    "            'priority_num', 'nunique')] > 1]\n",
    "\n",
    "        if len(inconsistent_combinations) > 0:\n",
    "            print(\n",
    "                f\"\\nInconsistent combinations found: {len(inconsistent_combinations)}\"\n",
    "            )\n",
    "            print(\"Priority variance by Impact-Urgency combinations:\")\n",
    "            print(inconsistent_combinations.head())\n",
    "        else:\n",
    "            print(\n",
    "                \"\\nAll Impact-Urgency combinations have consistent Priority assignments.\"\n",
    "            )\n",
    "\n",
    "        return priority_matrix\n",
    "    else:\n",
    "        print(\"Insufficient columns for Priority pattern analysis.\")\n",
    "        # Check available columns\n",
    "        available_cols = [\n",
    "            col for col in ['impact_num', 'urgency_num', 'priority_num']\n",
    "            if col in latest_incidents.columns\n",
    "        ]\n",
    "        missing_cols = [\n",
    "            col for col in ['impact_num', 'urgency_num', 'priority_num']\n",
    "            if col not in latest_incidents.columns\n",
    "        ]\n",
    "        print(f\"Available columns: {available_cols}\")\n",
    "        print(f\"Missing columns: {missing_cols}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Execute actual pattern analysis\n",
    "priority_matrix = analyze_priority_patterns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fb659d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if available_classification:\n",
    "    n_vars = len(available_classification)\n",
    "    fig = plt.figure(figsize=(6 * n_vars, 12))\n",
    "\n",
    "    # Statistical results storage\n",
    "    sla_summaries = {}\n",
    "    distribution_summaries = {}\n",
    "    closed_time_summaries = {}\n",
    "\n",
    "    for i, var in enumerate(available_classification):\n",
    "        # 1. SLA compliance rate (based on made_sla)\n",
    "        plt.subplot(3, n_vars, i + 1)\n",
    "        var_sla = latest_incidents.groupby(var)['sla_met'].agg(\n",
    "            ['mean', 'count']).sort_values('mean')\n",
    "\n",
    "        # Store for later output\n",
    "        sla_summaries[var] = var_sla\n",
    "\n",
    "        colors = plt.cm.RdYlGn(var_sla['mean'])\n",
    "        bars = plt.bar(range(len(var_sla)),\n",
    "                       var_sla['mean'],\n",
    "                       color=colors,\n",
    "                       alpha=0.8)\n",
    "\n",
    "        # Clean up labels\n",
    "        clean_labels = [\n",
    "            str(idx).split(' - ')[1] if ' - ' in str(idx) else str(idx)\n",
    "            for idx in var_sla.index\n",
    "        ]\n",
    "\n",
    "        plt.xticks(range(len(var_sla)), clean_labels, rotation=45, ha='right')\n",
    "        plt.ylabel('SLA Compliance Rate')\n",
    "        plt.title(f'SLA Compliance by {var.title()}')\n",
    "        plt.ylim(0, 1)\n",
    "\n",
    "        # Add value labels\n",
    "        for j, (idx, data) in enumerate(var_sla.iterrows()):\n",
    "            plt.text(j,\n",
    "                     data['mean'] + 0.02,\n",
    "                     f'{data[\"mean\"]:.1%}\\n(n={data[\"count\"]})',\n",
    "                     ha='center',\n",
    "                     va='bottom',\n",
    "                     fontsize=9,\n",
    "                     weight='bold')\n",
    "\n",
    "        # 2. Distribution pie chart\n",
    "        plt.subplot(3, n_vars, i + 1 + n_vars)\n",
    "        var_counts = latest_incidents[var].value_counts()\n",
    "\n",
    "        # Store for later output\n",
    "        distribution_summaries[var] = var_counts\n",
    "\n",
    "        # Create pie chart with clean labels\n",
    "        clean_pie_labels = [\n",
    "            str(label).split(' - ')[1] if ' - ' in str(label) else str(label)\n",
    "            for label in var_counts.index\n",
    "        ]\n",
    "\n",
    "        colors_pie = plt.cm.Set3(np.linspace(0, 1, len(var_counts)))\n",
    "        plt.pie(var_counts.values,\n",
    "                labels=clean_pie_labels,\n",
    "                autopct='%1.1f%%',\n",
    "                colors=colors_pie,\n",
    "                startangle=90)\n",
    "        plt.title(f'{var.title()} Distribution')\n",
    "\n",
    "        # 3. Closed time analysis by SLA status\n",
    "        plt.subplot(3, n_vars, i + 1 + 2 * n_vars)\n",
    "\n",
    "        # Prepare data for box plot\n",
    "        var_values = latest_incidents[var].unique()\n",
    "        box_data = []\n",
    "        box_labels = []\n",
    "        box_colors = []\n",
    "\n",
    "        closed_time_data = {}\n",
    "        for val in sorted(var_values):\n",
    "            if pd.notna(val):\n",
    "                val_data = latest_incidents[latest_incidents[var] == val]\n",
    "                closed_time_data[val] = val_data\n",
    "\n",
    "                # SLA met cases\n",
    "                met_times = val_data[val_data['sla_met'] ==\n",
    "                                     1]['closed_time_hours']\n",
    "                if len(met_times) > 0:\n",
    "                    box_data.append(met_times)\n",
    "                    clean_val = str(val).split(' - ')[1] if ' - ' in str(\n",
    "                        val) else str(val)\n",
    "                    box_labels.append(f\"{clean_val}_Met\")\n",
    "                    box_colors.append('lightgreen')\n",
    "\n",
    "                # SLA breach cases\n",
    "                breach_times = val_data[val_data['sla_met'] ==\n",
    "                                        0]['closed_time_hours']\n",
    "                if len(breach_times) > 0:\n",
    "                    box_data.append(breach_times)\n",
    "                    clean_val = str(val).split(' - ')[1] if ' - ' in str(\n",
    "                        val) else str(val)\n",
    "                    box_labels.append(f\"{clean_val}_Breach\")\n",
    "                    box_colors.append('lightcoral')\n",
    "\n",
    "        # Store for later output\n",
    "        closed_time_summaries[var] = closed_time_data\n",
    "\n",
    "        if box_data:\n",
    "            box_plot = plt.boxplot(box_data,\n",
    "                                   patch_artist=True,\n",
    "                                   showfliers=True)  # Show outliers\n",
    "            for patch, color in zip(box_plot['boxes'], box_colors):\n",
    "                patch.set_facecolor(color)\n",
    "                patch.set_alpha(0.7)\n",
    "\n",
    "            plt.xticks(range(1,\n",
    "                             len(box_data) + 1),\n",
    "                       box_labels,\n",
    "                       rotation=45,\n",
    "                       ha='right')\n",
    "            plt.ylabel('Closed Time (Hours)')\n",
    "            plt.title(f'{var.title()} Closed Times by SLA Status')\n",
    "        else:\n",
    "            plt.text(0.5,\n",
    "                     0.5,\n",
    "                     'Insufficient data for analysis',\n",
    "                     transform=plt.gca().transAxes,\n",
    "                     ha='center',\n",
    "                     va='center')\n",
    "            plt.title(f'{var.title()} Closed Times (Insufficient Data)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"DETAILED ANALYSIS\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    for var in available_classification:\n",
    "        if var in closed_time_summaries:\n",
    "            print(f\"\\n{var.title()} Closed Time Statistics:\")\n",
    "\n",
    "            for category, data in closed_time_summaries[var].items():\n",
    "                if len(data) > 0:\n",
    "                    # SLA Met cases\n",
    "                    met_times = data[data['sla_met'] ==\n",
    "                                     1]['closed_time_hours']\n",
    "                    breach_times = data[data['sla_met'] ==\n",
    "                                        0]['closed_time_hours']\n",
    "\n",
    "                    print(f\"\\n  {category}:\")\n",
    "\n",
    "                    if len(met_times) > 0:\n",
    "                        print(f\"    SLA Met (n={len(met_times)}):\")\n",
    "                        print(\n",
    "                            f\"      - Median: {met_times.median():.1f}h ({met_times.median()/24:.1f} days)\"\n",
    "                        )\n",
    "                        print(\n",
    "                            f\"      - Mean: {met_times.mean():.1f}h ({met_times.mean()/24:.1f} days)\"\n",
    "                        )\n",
    "                        print(\n",
    "                            f\"      - Max: {met_times.max():.1f}h ({met_times.max()/24:.1f} days)\"\n",
    "                        )\n",
    "                        print(\n",
    "                            f\"      - 95th percentile: {met_times.quantile(0.95):.1f}h\"\n",
    "                        )\n",
    "                        extreme_met = (met_times > 1000).sum()\n",
    "                        print(\n",
    "                            f\"      - Cases >1000h: {extreme_met} ({extreme_met/len(met_times):.1%})\"\n",
    "                        )\n",
    "\n",
    "                    if len(breach_times) > 0:\n",
    "                        print(f\"    SLA Breach (n={len(breach_times)}):\")\n",
    "                        print(\n",
    "                            f\"      - Median: {breach_times.median():.1f}h ({breach_times.median()/24:.1f} days)\"\n",
    "                        )\n",
    "                        print(\n",
    "                            f\"      - Mean: {breach_times.mean():.1f}h ({breach_times.mean()/24:.1f} days)\"\n",
    "                        )\n",
    "                        print(\n",
    "                            f\"      - Max: {breach_times.max():.1f}h ({breach_times.max()/24:.1f} days)\"\n",
    "                        )\n",
    "                        print(\n",
    "                            f\"      - 95th percentile: {breach_times.quantile(0.95):.1f}h\"\n",
    "                        )\n",
    "                        extreme_breach = (breach_times > 1000).sum()\n",
    "                        print(\n",
    "                            f\"      - Cases >1000h: {extreme_breach} ({extreme_breach/len(breach_times):.1%})\"\n",
    "                        )\n",
    "\n",
    "                        # Variance ratio\n",
    "                        if len(met_times) > 0:\n",
    "                            variance_ratio = breach_times.var(\n",
    "                            ) / met_times.var() if met_times.var(\n",
    "                            ) > 0 else float('inf')\n",
    "                            print(\n",
    "                                f\"      - Variance ratio (Breach/Met): {variance_ratio:.1f}x\"\n",
    "                            )\n",
    "\n",
    "        # Overall extreme cases analysis\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"EXTREME CASES ANALYSIS\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        total_incidents = len(latest_incidents)\n",
    "        extreme_1000h = latest_incidents[\n",
    "            latest_incidents['closed_time_hours'] > 1000]\n",
    "        extreme_5000h = latest_incidents[\n",
    "            latest_incidents['closed_time_hours'] > 5000]\n",
    "        extreme_8000h = latest_incidents[\n",
    "            latest_incidents['closed_time_hours'] > 8000]\n",
    "\n",
    "        print(f\"Total incidents analyzed: {total_incidents:,}\")\n",
    "        print(\n",
    "            f\"Cases >1000h (42 days): {len(extreme_1000h):,} ({len(extreme_1000h)/total_incidents:.2%})\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Cases >5000h (208 days): {len(extreme_5000h):,} ({len(extreme_5000h)/total_incidents:.2%})\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Cases >8000h (333 days): {len(extreme_8000h):,} ({len(extreme_8000h)/total_incidents:.2%})\"\n",
    "        )\n",
    "else:\n",
    "    print(\"No classification variables available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511478be",
   "metadata": {},
   "source": [
    " **Key Performance Insights** \n",
    "- Priority Performance Paradox: High-priority incidents achieved only 53.3% SLA compliance compared to 95.5% for low-priority incidents, creating a 42.2% performance gap that suggests either resource constraints for complex issues or unrealistic SLA targets.\n",
    "- Consistent Cross-Variable Patterns: Impact and urgency show similar inverse relationships (High impact: 51.6%, High urgency: 56.8% vs Low levels: ~95%). They exhibit comparable performance gaps of 44.2% and 38.0% respectively, confirming systematic resource allocation issues across all classification dimensions.\n",
    "- Classification Concentration Phenomenon: 93-95% of incidents cluster in medium/moderate categories while high/urgent represent only 1-3% of volume. This suggests conservative classification practices that may obscure true business impact and create resource allocation inefficiencies for genuinely critical cases.\n",
    "- Extended Closure Patterns: Analysis reveals 84.75% of all incidents exceed 42 days (1000 hours) for closure, with median times around 125-155 days, suggesting this dataset represents complex projects or major changes rather than typical IT incidents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970ea1b2",
   "metadata": {},
   "source": [
    "#### (3) Boolean Process Variables Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22422632",
   "metadata": {},
   "source": [
    "This analysis quantitatively evaluates how binary process decisions (knowledge consultation, priority confirmation, notifications) influence SLA compliance and operational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c30c27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define boolean process variables for analysis\n",
    "boolean_vars = ['knowledge', 'u_priority_confirmation', 'notify']\n",
    "available_boolean = [\n",
    "   var for var in boolean_vars if var in latest_incidents.columns\n",
    "]\n",
    "\n",
    "print(f\"Available boolean variables: {available_boolean}\")\n",
    "print(f\"Total incidents for analysis: {len(latest_incidents):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1bea4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if available_boolean:\n",
    "    # Enhanced boolean variable visualization\n",
    "    n_vars = len(available_boolean)\n",
    "    plt.figure(figsize=(6 * n_vars, 12))\n",
    "\n",
    "    # Statistical results storage\n",
    "    boolean_sla_summaries = {}\n",
    "    boolean_distribution_summaries = {}\n",
    "\n",
    "    for i, var in enumerate(available_boolean):\n",
    "        # Convert boolean values to consistent format\n",
    "        latest_incidents[f'{var}_clean'] = latest_incidents[var].map({\n",
    "            True:\n",
    "            'Yes',\n",
    "            'TRUE':\n",
    "            'Yes',\n",
    "            't':\n",
    "            'Yes',\n",
    "            'T':\n",
    "            'Yes',\n",
    "            1:\n",
    "            'Yes',\n",
    "            False:\n",
    "            'No',\n",
    "            'FALSE':\n",
    "            'No',\n",
    "            'f':\n",
    "            'No',\n",
    "            'F':\n",
    "            'No',\n",
    "            0:\n",
    "            'No',\n",
    "            'Do Not Notify':\n",
    "            'No',\n",
    "            'Send Email':\n",
    "            'Yes'\n",
    "        })\n",
    "\n",
    "        # 1. SLA compliance rate by boolean value\n",
    "        plt.subplot(3, n_vars, i + 1)\n",
    "        var_sla = latest_incidents.groupby(f'{var}_clean')['sla_met'].agg(\n",
    "            ['mean', 'count']).sort_values('mean')\n",
    "\n",
    "        # Store for later output\n",
    "        boolean_sla_summaries[var] = var_sla\n",
    "\n",
    "        # Color mapping based on performance\n",
    "        colors = [\n",
    "            'red' if x < 0.7 else 'orange' if x < 0.8 else 'green'\n",
    "            for x in var_sla['mean']\n",
    "        ]\n",
    "        bars = plt.bar(range(len(var_sla)),\n",
    "                       var_sla['mean'],\n",
    "                       color=colors,\n",
    "                       alpha=0.8)\n",
    "\n",
    "        plt.xticks(range(len(var_sla)), var_sla.index, rotation=45, ha='right')\n",
    "        plt.ylabel('SLA Compliance Rate')\n",
    "        plt.title(f'SLA Compliance by {var.replace(\"_\", \" \").title()}')\n",
    "        plt.ylim(0, 1)\n",
    "\n",
    "        # Add value labels\n",
    "        for j, (idx, data) in enumerate(var_sla.iterrows()):\n",
    "            plt.text(j,\n",
    "                     data['mean'] + 0.02,\n",
    "                     f'{data[\"mean\"]:.1%}\\n(n={data[\"count\"]:,})',\n",
    "                     ha='center',\n",
    "                     va='bottom',\n",
    "                     fontsize=9,\n",
    "                     weight='bold')\n",
    "\n",
    "        # 2. Distribution pie chart\n",
    "        plt.subplot(3, n_vars, i + 1 + n_vars)\n",
    "        var_counts = latest_incidents[f'{var}_clean'].value_counts()\n",
    "\n",
    "        # Store for later output\n",
    "        boolean_distribution_summaries[var] = var_counts\n",
    "\n",
    "        # Create pie chart\n",
    "        colors_pie = ['lightgreen', 'lightcoral',\n",
    "                      'lightgray'][:len(var_counts)]\n",
    "        plt.pie(var_counts.values,\n",
    "                labels=var_counts.index,\n",
    "                autopct='%1.1f%%',\n",
    "                colors=colors_pie,\n",
    "                startangle=90)\n",
    "        plt.title(f'{var.replace(\"_\", \" \").title()} Distribution')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Detailed Boolean Variable Analysis\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"DETAILED ANALYSIS\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Process quality impact analysis\n",
    "    for var in available_boolean:\n",
    "        if var in boolean_sla_summaries:\n",
    "            var_data = boolean_sla_summaries[var]\n",
    "\n",
    "            print(f\"\\n{var.replace('_', ' ').title()}:\")\n",
    "\n",
    "            # Show all categories and their performance\n",
    "            for category, data in var_data.iterrows():\n",
    "                print(\n",
    "                    f\"  - {category}: {data['mean']:.1%} SLA compliance (n={data['count']:,})\"\n",
    "                )\n",
    "\n",
    "            # Calculate and show performance differences\n",
    "            if 'Yes' in var_data.index and 'No' in var_data.index:\n",
    "                yes_performance = var_data.loc['Yes', 'mean']\n",
    "                no_performance = var_data.loc['No', 'mean']\n",
    "                performance_impact = yes_performance - no_performance\n",
    "\n",
    "                print(f\"  - Performance Impact: {performance_impact:+.1%}\")\n",
    "\n",
    "            # Distribution insights\n",
    "            if var in boolean_distribution_summaries:\n",
    "                dist_data = boolean_distribution_summaries[var]\n",
    "                total_cases = dist_data.sum()\n",
    "                print(f\"  - Usage Pattern:\")\n",
    "                for category, count in dist_data.items():\n",
    "                    percentage = (count / total_cases) * 100\n",
    "                    print(\n",
    "                        f\"    * {category}: {count:,} cases ({percentage:.1f}%)\"\n",
    "                    )\n",
    "\n",
    "else:\n",
    "    print(\"No boolean variables available for analysis\")\n",
    "    print(\"Available columns:\", [\n",
    "        col for col in latest_incidents.columns if col.lower() in\n",
    "        ['knowledge', 'notify', 'active', 'u_priority_confirmation']\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6305cc25",
   "metadata": {},
   "source": [
    "**Key Performance Insights**\n",
    "\n",
    "- Knowledge Enhancement Effect: Incidents requiring knowledge base consultation show 4.5% better SLA performance (90.2% vs 85.7%), suggesting that knowledge resources effectively support incident closure despite being used in only 15.5% of cases.\n",
    "- Priority Confirmation Paradox: Cases with priority confirmation achieve 14.7% worse SLA compliance (83.7% vs 98.4%), indicating that priority confirmation may be triggered by more complex cases requiring additional validation, though it's standardized across 81.6% of incidents.\n",
    "- Notification Uniformity: All incidents in the dataset show \"Do Not Notify\" status (100.0%), indicating a consistent notification policy with no variation to analyze.\n",
    "- Process Quality Impact: Knowledge utilization and priority confirmation processes create measurable SLA performance differences, confirming substantial impact on organizational operational efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b00c8d",
   "metadata": {},
   "source": [
    "#### (4) Assignment Group Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d274d9",
   "metadata": {},
   "source": [
    "This analysis examines organizational performance variations across different assignment groups to identify capability gaps, resource allocation patterns, and operational efficiency differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd272ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'assignment_group' in latest_incidents.columns:\n",
    "    try:\n",
    "        group_sla_performance = latest_incidents.groupby(\n",
    "            'assignment_group').agg({\n",
    "                'sla_met': ['count', 'mean', 'std'],\n",
    "                'closed_time_hours':\n",
    "                'mean',\n",
    "                'reassignment_count':\n",
    "                'mean' if 'reassignment_count' in latest_incidents.columns else\n",
    "                lambda x: np.nan\n",
    "            }).round(3)\n",
    "\n",
    "        group_sla_performance.columns = [\n",
    "            'incident_count', 'sla_compliance_rate', 'sla_std',\n",
    "            'avg_closed', 'avg_reassignments'\n",
    "        ]\n",
    "\n",
    "        # Filter groups with sufficient volume for reliable organizational performance comparison\n",
    "        # Small groups (< 20 incidents) may show extreme percentages due to sample size limitations\n",
    "        significant_groups = group_sla_performance[group_sla_performance['incident_count'] >= 20]\n",
    "\n",
    "        if len(significant_groups) > 0:\n",
    "            # Enhanced assignment group visualization\n",
    "            plt.figure(figsize=(20, 12))\n",
    "\n",
    "            # 1. SLA compliance ranking - Identifies best practices and training needs\n",
    "            plt.subplot(2, 4, 1)\n",
    "            top_performers = significant_groups.nlargest(\n",
    "                10, 'sla_compliance_rate')\n",
    "            colors = plt.cm.RdYlGn(top_performers['sla_compliance_rate'])\n",
    "            bars = plt.barh(range(len(top_performers)),\n",
    "                            top_performers['sla_compliance_rate'],\n",
    "                            color=colors)\n",
    "            plt.yticks(range(len(top_performers)),\n",
    "                       [name[:15] for name in top_performers.index])\n",
    "            plt.xlabel('SLA Compliance Rate')\n",
    "            plt.title('Top 10 SLA Performing Groups')\n",
    "\n",
    "            for i, (idx, rate) in enumerate(\n",
    "                    top_performers['sla_compliance_rate'].items()):\n",
    "                plt.text(rate + 0.01, i, f'{rate:.1%}', va='center')\n",
    "\n",
    "            # 2. Volume vs SLA performance scatter - Resource allocation insights\n",
    "            plt.subplot(2, 4, 2)\n",
    "            scatter = plt.scatter(significant_groups['incident_count'],\n",
    "                                  significant_groups['sla_compliance_rate'],\n",
    "                                  s=significant_groups['avg_reassignments'] *\n",
    "                                  50,\n",
    "                                  alpha=0.6,\n",
    "                                  c=significant_groups['avg_closed'],\n",
    "                                  cmap='viridis')\n",
    "            plt.xlabel('Incident Volume')\n",
    "            plt.ylabel('SLA Compliance Rate')\n",
    "            plt.title(\n",
    "                'Volume vs SLA Performance\\n(Size=Reassignments, Color=Avg Closed)'\n",
    "            )\n",
    "            plt.colorbar(scatter, label='Avg Closed Time (Hours)')\n",
    "\n",
    "            # 3. SLA consistency analysis - Identifies groups with unstable performance\n",
    "            plt.subplot(2, 4, 3)\n",
    "            plt.scatter(significant_groups['sla_compliance_rate'],\n",
    "                        significant_groups['sla_std'],\n",
    "                        s=significant_groups['incident_count'] * 0.5,\n",
    "                        alpha=0.6,\n",
    "                        color='orange')\n",
    "            plt.xlabel('SLA Compliance Rate')\n",
    "            plt.ylabel('SLA Standard Deviation')\n",
    "            plt.title('SLA Consistency Analysis\\n(Size = Volume)')\n",
    "\n",
    "            # 4. Worst performing groups - Immediate intervention targets\n",
    "            plt.subplot(2, 4, 4)\n",
    "            worst_performers = significant_groups.nsmallest(\n",
    "                10, 'sla_compliance_rate')\n",
    "            colors = plt.cm.Reds(1 - worst_performers['sla_compliance_rate'])\n",
    "            bars = plt.barh(range(len(worst_performers)),\n",
    "                            worst_performers['sla_compliance_rate'],\n",
    "                            color=colors)\n",
    "            plt.yticks(range(len(worst_performers)),\n",
    "                       [name[:15] for name in worst_performers.index])\n",
    "            plt.xlabel('SLA Compliance Rate')\n",
    "            plt.title('Bottom 10 SLA Performing Groups')\n",
    "\n",
    "            for i, (idx, rate) in enumerate(\n",
    "                    worst_performers['sla_compliance_rate'].items()):\n",
    "                plt.text(rate + 0.01, i, f'{rate:.1%}', va='center')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            # Detailed Assignment Group Analysis\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(\"DETAILED ANALYSIS\")\n",
    "            print(f\"{'='*60}\")\n",
    "\n",
    "            # Overall statistics\n",
    "            print(f\"\\nOverall Group Performance Statistics:\")\n",
    "            print(f\"- Total groups analyzed: {len(significant_groups)}\")\n",
    "            print(\n",
    "                f\"- Average SLA compliance: {significant_groups['sla_compliance_rate'].mean():.1%}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"- Performance range: {significant_groups['sla_compliance_rate'].min():.1%} - {significant_groups['sla_compliance_rate'].max():.1%}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"- Performance gap: {significant_groups['sla_compliance_rate'].max() - significant_groups['sla_compliance_rate'].min():.1%}\"\n",
    "            )\n",
    "\n",
    "            # Volume analysis\n",
    "            print(f\"\\nVolume Distribution Analysis:\")\n",
    "            total_incidents = significant_groups['incident_count'].sum()\n",
    "            high_volume_threshold = significant_groups[\n",
    "                'incident_count'].quantile(0.8)\n",
    "            high_volume_groups = significant_groups[\n",
    "                significant_groups['incident_count'] >= high_volume_threshold]\n",
    "\n",
    "            print(f\"- Total incidents handled: {total_incidents:,}\")\n",
    "            print(\n",
    "                f\"- High-volume groups (top 20%): {len(high_volume_groups)} groups handling {high_volume_groups['incident_count'].sum():,} incidents\"\n",
    "            )\n",
    "            print(\n",
    "                f\"- High-volume group avg performance: {high_volume_groups['sla_compliance_rate'].mean():.1%}\"\n",
    "            )\n",
    "\n",
    "            # Efficiency analysis\n",
    "            print(f\"\\nOperational Efficiency Analysis:\")\n",
    "            if 'avg_reassignments' in significant_groups.columns and significant_groups[\n",
    "                    'avg_reassignments'].notna().any():\n",
    "                avg_reassignments = significant_groups[\n",
    "                    'avg_reassignments'].mean()\n",
    "                print(f\"- Average reassignment rate: {avg_reassignments:.2f}\")\n",
    "\n",
    "                low_reassign_groups = significant_groups[\n",
    "                    significant_groups['avg_reassignments'] <=\n",
    "                    avg_reassignments]\n",
    "                print(\n",
    "                    f\"- Groups with below-average reassignments: {len(low_reassign_groups)} ({len(low_reassign_groups)/len(significant_groups):.1%})\"\n",
    "                )\n",
    "                print(\n",
    "                    f\"- Their average SLA performance: {low_reassign_groups['sla_compliance_rate'].mean():.1%}\"\n",
    "                )\n",
    "\n",
    "            # closed time insights\n",
    "            print(f\"\\nClosed Time Patterns:\")\n",
    "            print(\n",
    "                f\"- Average closed time: {significant_groups['avg_closed'].mean():.1f} hours\"\n",
    "            )\n",
    "            print(\n",
    "                f\"- Fastest average closed: {significant_groups['avg_closed'].min():.1f} hours\"\n",
    "            )\n",
    "            print(\n",
    "                f\"- Slowest average closed: {significant_groups['avg_closed'].max():.1f} hours\"\n",
    "            )\n",
    "\n",
    "            # Top and bottom performer analysis\n",
    "            print(f\"\\nPerformance Tier Analysis:\")\n",
    "            top_tier = significant_groups.nlargest(5, 'sla_compliance_rate')\n",
    "            bottom_tier = significant_groups.nsmallest(5,\n",
    "                                                       'sla_compliance_rate')\n",
    "\n",
    "            print(\n",
    "                f\"- Top 5 groups average: {top_tier['sla_compliance_rate'].mean():.1%} SLA compliance\"\n",
    "            )\n",
    "            print(\n",
    "                f\"- Bottom 5 groups average: {bottom_tier['sla_compliance_rate'].mean():.1%} SLA compliance\"\n",
    "            )\n",
    "            print(\n",
    "                f\"- Top tier handles {top_tier['incident_count'].sum():,} incidents ({top_tier['incident_count'].sum()/total_incidents:.1%} of total)\"\n",
    "            )\n",
    "            print(\n",
    "                f\"- Bottom tier handles {bottom_tier['incident_count'].sum():,} incidents ({bottom_tier['incident_count'].sum()/total_incidents:.1%} of total)\"\n",
    "            )\n",
    "\n",
    "            # Print top and worst performers for actionable insights\n",
    "            print(f\"\\nTop 5 SLA Performing Groups:\")\n",
    "            print(\n",
    "                \"Group | Incidents | SLA Rate | Avg Closed | Reassignments\"\n",
    "            )\n",
    "            print(\"-\" * 65)\n",
    "            for group, data in top_performers.head(5).iterrows():\n",
    "                print(\n",
    "                    f\"{group[:15]:15} | {data['incident_count']:9} | {data['sla_compliance_rate']:8.1%} | {data['avg_closed']:13.1f}h | {data['avg_reassignments']:11.2f}\"\n",
    "                )\n",
    "\n",
    "            print(f\"\\nWorst 5 SLA Performing Groups:\")\n",
    "            print(\n",
    "                \"Group | Incidents | SLA Rate | Avg Closed | Reassignments\"\n",
    "            )\n",
    "            print(\"-\" * 65)\n",
    "            for group, data in worst_performers.head(5).iterrows():\n",
    "                print(\n",
    "                    f\"{group[:15]:15} | {data['incident_count']:9} | {data['sla_compliance_rate']:8.1%} | {data['avg_closed']:13.1f}h | {data['avg_reassignments']:11.2f}\"\n",
    "                )\n",
    "        else:\n",
    "            print(\"No assignment groups with sufficient volume for analysis\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in assignment group analysis: {e}\")\n",
    "else:\n",
    "    print(\"Assignment group column not available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef62f0eb",
   "metadata": {},
   "source": [
    "**Key Performance Insights**\n",
    "- Extreme Performance Variance: Analysis reveals significant performance disparity across assignment groups, with top performer (Group 64: 100.0%) substantially outperforming worst performer (Group 3: 44.1%) - a 55.9% gap indicating major capability differences across teams.\n",
    "- Volume-Performance Balance: High-volume groups in the top 20% handle 4,185 incidents (71.3% of total) while maintaining 88.1% average performance, demonstrating effective scalability for major service teams.\n",
    "- Operational Efficiency Correlation: Top performers show superior efficiency with low reassignment rates (Group 64: 0.37 reassignments) compared to struggling groups (Group 3: 1.79 reassignments), suggesting process maturity differences.\n",
    "- Performance Concentration: Top 5 groups average 96.6% SLA compliance while bottom 5 average only 51.5%, with top tier handling relatively low volume (5.6% of incidents), indicating specialized high-performing units versus struggling generalist teams."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b95ae2",
   "metadata": {},
   "source": [
    "#### (5) Category and Subcategory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9336a3e4",
   "metadata": {},
   "source": [
    "This analysis identifies service domains with the highest risk profiles and performance variations to prioritize improvement efforts and resource allocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5944f0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'category' in latest_incidents.columns:\n",
    "    try:\n",
    "        category_sla = latest_incidents.groupby('category').agg({\n",
    "            'sla_met': ['count', 'mean'],\n",
    "            'closed_time_hours': ['mean', 'std'],\n",
    "            'reassignment_count':\n",
    "            'mean' if 'reassignment_count' in latest_incidents.columns else\n",
    "            lambda x: np.nan\n",
    "        }).round(3)\n",
    "\n",
    "        category_sla.columns = [\n",
    "            'volume', 'sla_rate', 'avg_closed', 'closed_std',\n",
    "            'avg_reassignments'\n",
    "        ]\n",
    "        significant_categories = category_sla\n",
    "\n",
    "        if len(significant_categories) > 0:\n",
    "            # Enhanced category visualization\n",
    "            plt.figure(figsize=(20, 12))\n",
    "\n",
    "            # 1. Most problematic categories - Priority intervention list\n",
    "            plt.subplot(2, 2, 1)\n",
    "            # Risk score: (1 - sla_rate) * volume - Combines impact and frequency\n",
    "            significant_categories['risk_score'] = (\n",
    "                1 - significant_categories['sla_rate']\n",
    "            ) * significant_categories['volume']\n",
    "            top_risk_categories = significant_categories.nlargest(\n",
    "                10, 'risk_score')\n",
    "\n",
    "            bars = plt.barh(range(len(top_risk_categories)),\n",
    "                            top_risk_categories['risk_score'],\n",
    "                            color=plt.cm.Reds(\n",
    "                                np.linspace(0.3, 0.9,\n",
    "                                            len(top_risk_categories))))\n",
    "\n",
    "            # Show actual category names\n",
    "            plt.yticks(range(len(top_risk_categories)),\n",
    "                       [str(cat) for cat in top_risk_categories.index],\n",
    "                       fontsize=8)\n",
    "            plt.xlabel('Risk Score (Breach Rate  Volume)')\n",
    "            plt.title('Highest Risk Categories')\n",
    "\n",
    "            # 2. Best performing categories - Best practice examples\n",
    "            plt.subplot(2, 2, 2)\n",
    "            best_categories = significant_categories.nlargest(10, 'sla_rate')\n",
    "            bars = plt.barh(range(len(best_categories)),\n",
    "                            best_categories['sla_rate'],\n",
    "                            color=plt.cm.Greens(\n",
    "                                np.linspace(0.3, 0.9, len(best_categories))))\n",
    "\n",
    "            # Show actual category names\n",
    "            plt.yticks(range(len(best_categories)),\n",
    "                       [str(cat) for cat in best_categories.index],\n",
    "                       fontsize=8)\n",
    "            plt.xlabel('SLA Compliance Rate')\n",
    "            plt.title('Best Performing Categories')\n",
    "\n",
    "            # 3. Worst Category-Subcategory combinations - Specific improvement targets\n",
    "            if 'subcategory' in latest_incidents.columns:\n",
    "                plt.subplot(2, 2, 3)\n",
    "\n",
    "                # Create category-subcategory combinations\n",
    "                latest_incidents['cat_subcat'] = (\n",
    "                    latest_incidents['category'].astype(str).str.replace(\n",
    "                        ' ', '') + '\\n' +\n",
    "                    latest_incidents['subcategory'].astype(str).str.replace(\n",
    "                        ' ', ''))\n",
    "\n",
    "                combo_sla = latest_incidents.groupby('cat_subcat').agg({\n",
    "                    'sla_met': ['count', 'mean']\n",
    "                }).round(3)\n",
    "                combo_sla.columns = ['volume', 'sla_rate']\n",
    "                combo_filtered = combo_sla[combo_sla['volume'] >= 10]\n",
    "\n",
    "                if len(combo_filtered) > 0:\n",
    "                    # Most problematic combinations\n",
    "                    worst_combos = combo_filtered.nsmallest(10, 'sla_rate')\n",
    "                    bars = plt.barh(range(len(worst_combos)),\n",
    "                                    worst_combos['sla_rate'],\n",
    "                                    color=plt.cm.Reds(\n",
    "                                        np.linspace(0.3, 0.9,\n",
    "                                                    len(worst_combos))))\n",
    "\n",
    "                    # Show actual combination names\n",
    "                    plt.yticks(range(len(worst_combos)), [\n",
    "                        combo[:25] + '...' if len(combo) > 25 else combo\n",
    "                        for combo in worst_combos.index\n",
    "                    ],\n",
    "                               fontsize=6)\n",
    "                    plt.xlabel('SLA Compliance Rate')\n",
    "                    plt.title('Worst Category-Subcategory Combinations')\n",
    "                else:\n",
    "                    plt.text(0.5,\n",
    "                             0.5,\n",
    "                             'Insufficient data for\\ncombination analysis',\n",
    "                             transform=plt.gca().transAxes,\n",
    "                             ha='center')\n",
    "                    plt.title('Category-Subcategory Analysis (N/A)')\n",
    "            else:\n",
    "                plt.text(0.5,\n",
    "                         0.5,\n",
    "                         'Subcategory not available',\n",
    "                         transform=plt.gca().transAxes,\n",
    "                         ha='center')\n",
    "                plt.title('Category-Subcategory Analysis (N/A)')\n",
    "\n",
    "            # 4. Best Category-Subcategory combinations\n",
    "            if 'subcategory' in latest_incidents.columns and 'combo_filtered' in locals(\n",
    "            ) and len(combo_filtered) > 0:\n",
    "                plt.subplot(2, 2, 4)\n",
    "\n",
    "                # Best performing combinations\n",
    "                best_combos = combo_filtered.nlargest(10, 'sla_rate')\n",
    "                bars = plt.barh(range(len(best_combos)),\n",
    "                                best_combos['sla_rate'],\n",
    "                                color=plt.cm.Greens(\n",
    "                                    np.linspace(0.3, 0.9, len(best_combos))))\n",
    "\n",
    "                # Show actual combination names\n",
    "                plt.yticks(range(len(best_combos)), [\n",
    "                    combo[:25] + '...' if len(combo) > 25 else combo\n",
    "                    for combo in best_combos.index\n",
    "                ],\n",
    "                           fontsize=6)\n",
    "                plt.xlabel('SLA Compliance Rate')\n",
    "                plt.title('Best Category-Subcategory Combinations')\n",
    "            else:\n",
    "                plt.text(\n",
    "                    0.5,\n",
    "                    0.5,\n",
    "                    'Subcategory not available\\nfor best combination analysis',\n",
    "                    transform=plt.gca().transAxes,\n",
    "                    ha='center')\n",
    "                plt.title('Best Combination Analysis (N/A)')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            # Detailed Category Analysis\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(\"DETAILED ANALYSIS\")\n",
    "            print(f\"{'='*60}\")\n",
    "\n",
    "            # Overall category statistics\n",
    "            print(f\"\\nOverall Category Performance Statistics:\")\n",
    "            print(\n",
    "                f\"- Total categories analyzed: {len(significant_categories)}\")\n",
    "            print(\n",
    "                f\"- Average SLA compliance: {significant_categories['sla_rate'].mean():.1%}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"- Performance range: {significant_categories['sla_rate'].min():.1%} - {significant_categories['sla_rate'].max():.1%}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"- Performance gap: {significant_categories['sla_rate'].max() - significant_categories['sla_rate'].min():.1%}\"\n",
    "            )\n",
    "\n",
    "            # Top risk categories details\n",
    "            print(f\"\\nTop 5 Risk Categories (Immediate Attention Required):\")\n",
    "            print(\"Category | Volume | SLA Rate | Avg Closed | Risk Score\")\n",
    "            print(\"-\" * 70)\n",
    "            for cat, data in top_risk_categories.head(5).iterrows():\n",
    "                cat_name = str(cat)[:15] + \"...\" if len(\n",
    "                    str(cat)) > 15 else str(cat)\n",
    "                print(\n",
    "                    f\"{cat_name:15} | {data['volume']:6} | {data['sla_rate']:8.1%} | {data['avg_closed']:13.1f}h | {data['risk_score']:10.1f}\"\n",
    "                )\n",
    "\n",
    "            # Best performing categories\n",
    "            print(f\"\\nTop 5 Best Performing Categories:\")\n",
    "            print(\n",
    "                \"Category | Volume | SLA Rate | Avg closed | Reassignments\"\n",
    "            )\n",
    "            print(\"-\" * 70)\n",
    "            for cat, data in best_categories.head(5).iterrows():\n",
    "                cat_name = str(cat)[:15] + \"...\" if len(\n",
    "                    str(cat)) > 15 else str(cat)\n",
    "                reassign_val = data['avg_reassignments'] if pd.notna(\n",
    "                    data['avg_reassignments']) else 0\n",
    "                print(\n",
    "                    f\"{cat_name:15} | {data['volume']:6} | {data['sla_rate']:8.1%} | {data['avg_closed']:13.1f}h | {reassign_val:11.2f}\"\n",
    "                )\n",
    "\n",
    "            # Worst subcategory combination analysis\n",
    "            if 'subcategory' in latest_incidents.columns and 'combo_filtered' in locals(\n",
    "            ) and len(combo_filtered) > 0:\n",
    "                print(f\"\\nWorst Category-Subcategory Combinations:\")\n",
    "                print(\"Combination | Volume | SLA Rate\")\n",
    "                print(\"-\" * 50)\n",
    "                for combo, data in worst_combos.head(5).iterrows():\n",
    "                    combo_short = combo[:35] + \"...\" if len(\n",
    "                        combo) > 35 else combo\n",
    "                    print(\n",
    "                        f\"{combo_short:35} | {data['volume']:6} | {data['sla_rate']:8.1%}\"\n",
    "                    )\n",
    "\n",
    "                # Best subcategory combination analysis\n",
    "                print(\n",
    "                    f\"\\nBest Category-Subcategory Combinations (Best Practices):\"\n",
    "                )\n",
    "                print(\"Combination | Volume | SLA Rate\")\n",
    "                print(\"-\" * 50)\n",
    "                for combo, data in best_combos.head(5).iterrows():\n",
    "                    combo_short = combo[:35] + \"...\" if len(\n",
    "                        combo) > 35 else combo\n",
    "                    print(\n",
    "                        f\"{combo_short:35} | {data['volume']:6} | {data['sla_rate']:8.1%}\"\n",
    "                    )\n",
    "\n",
    "        else:\n",
    "            print(\"No categories with sufficient volume for analysis\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in category analysis: {e}\")\n",
    "else:\n",
    "    print(\"Category column not available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0257c5b4",
   "metadata": {},
   "source": [
    "**Key Performance Insights**\n",
    "- Extreme Category Performance Disparity: 46 categories show 62.5% performance gap between best (100% SLA) and worst (37.5% SLA) performers, indicating severe technical complexity and process maturity variations across service domains.\n",
    "- High-Risk Category Identification: Category 46 emerges as the highest risk with score 134.9 (579 incidents, 76.7% SLA), followed by Category 26 (93.8 risk score), requiring immediate intervention and resource allocation.\n",
    "- Volume-Performance Concentration: High-volume categories like Category 42 (943 incidents) and Category 26 (877 incidents) handle substantial workloads while maintaining 92.4% and 89.3% SLA respectively, demonstrating operational scalability.\n",
    "- Subcategory Combination Critical Impact: Within Category 46, subcategory combinations range from 50.0% to potentially 100% SLA performance, proving that granular process optimization within categories is the critical performance differentiator.\n",
    "- Small Volume Perfection vs Scale Challenges: Small categories (1-3 incidents) achieve perfect 100% SLA, while larger categories face performance pressures, highlighting the challenge of maintaining quality at scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5563705f",
   "metadata": {},
   "source": [
    "#### (6) Temporal Pattern Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddd51ed",
   "metadata": {},
   "source": [
    "This analysis examines time-based performance variations to identify operational efficiency patterns, resource allocation opportunities, and seasonal trends affecting SLA compliance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7660a5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'opened_at' in latest_incidents.columns:\n",
    "    try:\n",
    "        latest_incidents['opened_hour'] = latest_incidents['opened_at'].dt.hour\n",
    "        latest_incidents['opened_day_of_week'] = latest_incidents[\n",
    "            'opened_at'].dt.dayofweek\n",
    "        latest_incidents['opened_month'] = latest_incidents[\n",
    "            'opened_at'].dt.month\n",
    "        latest_incidents['opened_quarter'] = latest_incidents[\n",
    "            'opened_at'].dt.quarter\n",
    "        latest_incidents['is_business_hours'] = latest_incidents[\n",
    "            'opened_hour'].between(9, 17)\n",
    "        latest_incidents['is_weekend'] = latest_incidents[\n",
    "            'opened_day_of_week'].isin([5, 6])\n",
    "\n",
    "        # Calculate temporal patterns for feature engineering\n",
    "        hourly_sla = latest_incidents.groupby('opened_hour')['sla_met'].agg(\n",
    "            ['mean', 'count'])\n",
    "        daily_sla = latest_incidents.groupby(\n",
    "            'opened_day_of_week')['sla_met'].agg(['mean', 'count'])\n",
    "        monthly_sla = latest_incidents.groupby('opened_month')['sla_met'].agg(\n",
    "            ['mean', 'count'])\n",
    "        quarterly_sla = latest_incidents.groupby(\n",
    "            'opened_quarter')['sla_met'].agg(['mean', 'count', 'std'])\n",
    "        hourly_volume = latest_incidents.groupby('opened_hour').size()\n",
    "\n",
    "        # Business hours analysis for feature engineering\n",
    "        business_sla = latest_incidents.groupby('is_business_hours').agg({\n",
    "            'sla_met': ['mean', 'count'],\n",
    "            'closed_time_hours':\n",
    "            'mean' if 'closed_time_hours' in latest_incidents.columns else\n",
    "            lambda x: np.nan\n",
    "        }).round(3)\n",
    "        business_sla.columns = ['sla_rate', 'volume', 'avg_closed']\n",
    "\n",
    "        # Weekend analysis\n",
    "        weekend_sla = latest_incidents.groupby('is_weekend').agg({\n",
    "            'sla_met': ['mean', 'count']\n",
    "        }).round(3)\n",
    "        weekend_sla.columns = ['sla_rate', 'volume']\n",
    "\n",
    "        plt.figure(figsize=(20, 16))\n",
    "\n",
    "        # 1. Daily SLA performance - Weekly staffing patterns\n",
    "        plt.subplot(3, 4, 1)\n",
    "        weekday_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "        colors = ['lightblue' if i < 5 else 'lightcoral' for i in range(7)]\n",
    "\n",
    "        bars = plt.bar(range(7), daily_sla['mean'], color=colors, alpha=0.8)\n",
    "        plt.xlabel('Day of Week')\n",
    "        plt.ylabel('SLA Compliance Rate')\n",
    "        plt.title('Daily SLA Performance Pattern')\n",
    "        plt.xticks(range(7), weekday_names)\n",
    "\n",
    "        # Add statistical annotations\n",
    "        for i, (mean_val,\n",
    "                count) in enumerate(zip(daily_sla['mean'],\n",
    "                                        daily_sla['count'])):\n",
    "            plt.text(i,\n",
    "                     mean_val + 0.02,\n",
    "                     f'{mean_val:.1%}\\n(n={count})',\n",
    "                     ha='center',\n",
    "                     va='bottom',\n",
    "                     fontsize=8)\n",
    "\n",
    "        # 2. Monthly SLA trend - Long-term pattern identification\n",
    "        plt.subplot(3, 4, 2)\n",
    "        plt.plot(monthly_sla.index,\n",
    "                 monthly_sla['mean'],\n",
    "                 marker='s',\n",
    "                 linewidth=2,\n",
    "                 color='green',\n",
    "                 markersize=6)\n",
    "\n",
    "        # Add trend line for forecasting\n",
    "        if len(monthly_sla) > 2:\n",
    "            z = np.polyfit(monthly_sla.index, monthly_sla['mean'], 1)\n",
    "            p = np.poly1d(z)\n",
    "            plt.plot(monthly_sla.index,\n",
    "                     p(monthly_sla.index),\n",
    "                     linestyle='--',\n",
    "                     color='red',\n",
    "                     alpha=0.7,\n",
    "                     label=f'Trend: {z[0]*12:.1%}/year')\n",
    "            plt.legend()\n",
    "\n",
    "        plt.xlabel('Month')\n",
    "        plt.ylabel('SLA Compliance Rate')\n",
    "        plt.title('Monthly SLA Trend')\n",
    "        plt.xticks(range(1, 13))\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        # 3. Quarterly seasonal analysis\n",
    "        plt.subplot(3, 4, 3)\n",
    "        quarters = ['Q1', 'Q2', 'Q3', 'Q4']\n",
    "        seasonal_colors = ['lightblue', 'yellow', 'orange', 'lightcoral']\n",
    "\n",
    "        bars = plt.bar(range(4),\n",
    "                       quarterly_sla['mean'],\n",
    "                       color=seasonal_colors,\n",
    "                       alpha=0.8,\n",
    "                       yerr=quarterly_sla['std'],\n",
    "                       capsize=5)\n",
    "        plt.xlabel('Quarter')\n",
    "        plt.ylabel('SLA Compliance Rate')\n",
    "        plt.title('Seasonal SLA Performance')\n",
    "        plt.xticks(range(4), quarters)\n",
    "\n",
    "        for i, (mean_val, count) in enumerate(\n",
    "                zip(quarterly_sla['mean'], quarterly_sla['count'])):\n",
    "            plt.text(i,\n",
    "                     mean_val + 0.05,\n",
    "                     f'{mean_val:.1%}\\n(n={count})',\n",
    "                     ha='center',\n",
    "                     va='bottom',\n",
    "                     fontsize=8)\n",
    "\n",
    "        # 4. Volume vs SLA by hour - Resource planning tool\n",
    "        plt.subplot(3, 4, 4)\n",
    "        ax1 = plt.gca()\n",
    "        color = 'tab:blue'\n",
    "        ax1.set_xlabel('Hour of Day')\n",
    "        ax1.set_ylabel('Incident Volume', color=color)\n",
    "        ax1.bar(hourly_volume.index,\n",
    "                hourly_volume.values,\n",
    "                alpha=0.6,\n",
    "                color=color)\n",
    "        ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "        ax2 = ax1.twinx()\n",
    "        color = 'tab:red'\n",
    "        ax2.set_ylabel('SLA Compliance Rate', color=color)\n",
    "        ax2.plot(hourly_sla.index,\n",
    "                 hourly_sla['mean'],\n",
    "                 color=color,\n",
    "                 marker='o',\n",
    "                 linewidth=2)\n",
    "        ax2.tick_params(axis='y', labelcolor=color)\n",
    "        plt.title('Volume vs SLA Performance by Hour')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Comprehensive temporal analysis and insights\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"DETAILED ANALYSIS\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        # Daily pattern analysis\n",
    "        best_day = daily_sla['mean'].idxmax()\n",
    "        worst_day = daily_sla['mean'].idxmin()\n",
    "        daily_gap = daily_sla['mean'].max() - daily_sla['mean'].min()\n",
    "\n",
    "        print(f\"\\nDaily Performance Patterns:\")\n",
    "        print(\n",
    "            f\"- Best performing day: {weekday_names[best_day]} ({daily_sla['mean'].iloc[best_day]:.1%} SLA)\"\n",
    "        )\n",
    "        print(\n",
    "            f\"- Worst performing day: {weekday_names[worst_day]} ({daily_sla['mean'].iloc[worst_day]:.1%} SLA)\"\n",
    "        )\n",
    "        print(f\"- Daily performance gap: {daily_gap:.1%}\")\n",
    "\n",
    "        # Weekend vs weekday analysis\n",
    "        if len(weekend_sla) >= 2:\n",
    "            weekend_effect = weekend_sla['sla_rate'].iloc[1] - weekend_sla[\n",
    "                'sla_rate'].iloc[0]\n",
    "            print(\n",
    "                f\"- Weekend performance boost: {weekend_effect:+.1%} compared to weekdays\"\n",
    "            )\n",
    "            print(\n",
    "                f\"- Weekend volume: {weekend_sla['volume'].iloc[1]:,} incidents\"\n",
    "            )\n",
    "            print(\n",
    "                f\"- Weekday volume: {weekend_sla['volume'].iloc[0]:,} incidents\"\n",
    "            )\n",
    "\n",
    "        # Monthly trend analysis\n",
    "        print(f\"\\nMonthly Trend Analysis:\")\n",
    "        best_month = monthly_sla['mean'].idxmax()\n",
    "        worst_month = monthly_sla['mean'].idxmin()\n",
    "        monthly_gap = monthly_sla['mean'].max() - monthly_sla['mean'].min()\n",
    "\n",
    "        print(\n",
    "            f\"- Best performing month: Month {best_month} ({monthly_sla['mean'].iloc[best_month-1]:.1%} SLA)\"\n",
    "        )\n",
    "        print(\n",
    "            f\"- Worst performing month: Month {worst_month} ({monthly_sla['mean'].iloc[worst_month-1]:.1%} SLA)\"\n",
    "        )\n",
    "        print(f\"- Monthly performance range: {monthly_gap:.1%}\")\n",
    "\n",
    "        if 'z' in locals():\n",
    "            annual_trend = z[0] * 12\n",
    "            trend_direction = \"improving\" if annual_trend > 0 else \"declining\"\n",
    "            print(\n",
    "                f\"- Annual SLA trend: {annual_trend:+.1%} per year ({trend_direction})\"\n",
    "            )\n",
    "\n",
    "        # Quarterly seasonal analysis\n",
    "        print(f\"\\nQuarterly Seasonal Effects:\")\n",
    "        best_quarter = quarterly_sla['mean'].idxmax()\n",
    "        worst_quarter = quarterly_sla['mean'].idxmin()\n",
    "        quarterly_gap = quarterly_sla['mean'].max(\n",
    "        ) - quarterly_sla['mean'].min()\n",
    "\n",
    "        q_names = ['Q1', 'Q2', 'Q3', 'Q4']\n",
    "        print(\n",
    "            f\"- Best performing quarter: {q_names[best_quarter-1]} ({quarterly_sla['mean'].iloc[best_quarter-1]:.1%} SLA)\"\n",
    "        )\n",
    "        print(\n",
    "            f\"- Worst performing quarter: {q_names[worst_quarter-1]} ({quarterly_sla['mean'].iloc[worst_quarter-1]:.1%} SLA)\"\n",
    "        )\n",
    "        print(f\"- Quarterly performance gap: {quarterly_gap:.1%}\")\n",
    "        print(\n",
    "            f\"- Q1 performance variance: {quarterly_sla['std'].iloc[0]:.3f} (budget/resource constraints)\"\n",
    "        )\n",
    "\n",
    "        # Hourly performance analysis\n",
    "        print(f\"\\nHourly Performance Dynamics:\")\n",
    "        best_hour = hourly_sla['mean'].idxmax()\n",
    "        worst_hour = hourly_sla['mean'].idxmin()\n",
    "        hourly_gap = hourly_sla['mean'].max() - hourly_sla['mean'].min()\n",
    "\n",
    "        print(\n",
    "            f\"- Best performing hour: {best_hour}:00 ({hourly_sla['mean'].iloc[best_hour]:.1%} SLA)\"\n",
    "        )\n",
    "        print(\n",
    "            f\"- Worst performing hour: {worst_hour}:00 ({hourly_sla['mean'].iloc[worst_hour]:.1%} SLA)\"\n",
    "        )\n",
    "        print(f\"- Hourly performance range: {hourly_gap:.1%}\")\n",
    "\n",
    "        # Peak hours analysis\n",
    "        peak_hours = hourly_volume.nlargest(6).index.tolist()\n",
    "        peak_sla_avg = hourly_sla.loc[peak_hours, 'mean'].mean()\n",
    "        off_peak_hours = [h for h in range(24) if h not in peak_hours]\n",
    "        off_peak_sla_avg = hourly_sla.loc[off_peak_hours, 'mean'].mean()\n",
    "\n",
    "        print(f\"- Peak volume hours: {peak_hours}\")\n",
    "        print(f\"- Peak hours avg SLA: {peak_sla_avg:.1%}\")\n",
    "        print(f\"- Off-peak hours avg SLA: {off_peak_sla_avg:.1%}\")\n",
    "        print(\n",
    "            f\"- Peak hour performance penalty: {peak_sla_avg - off_peak_sla_avg:+.1%}\"\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in temporal analysis: {e}\")\n",
    "        # Create dummy variables to prevent downstream errors\n",
    "        hourly_sla = pd.DataFrame({'mean': [0.8] * 24}, index=range(24))\n",
    "        daily_sla = pd.DataFrame({'mean': [0.8] * 7}, index=range(7))\n",
    "        hourly_volume = pd.Series([100] * 24, index=range(24))\n",
    "        business_sla = pd.DataFrame({\n",
    "            'sla_rate': [0.8, 0.8],\n",
    "            'volume': [1000, 3000]\n",
    "        })\n",
    "        weekend_sla = pd.DataFrame({\n",
    "            'sla_rate': [0.8, 0.85],\n",
    "            'volume': [15000, 8000]\n",
    "        })\n",
    "        print(\"Using default temporal variables for feature engineering\")\n",
    "\n",
    "else:\n",
    "    print(\"Date information not available for temporal analysis\")\n",
    "    # Create dummy variables to prevent downstream errors\n",
    "    hourly_sla = pd.DataFrame({'mean': [0.8] * 24}, index=range(24))\n",
    "    daily_sla = pd.DataFrame({'mean': [0.8] * 7}, index=range(7))\n",
    "    hourly_volume = pd.Series([100] * 24, index=range(24))\n",
    "    business_sla = pd.DataFrame({\n",
    "        'sla_rate': [0.8, 0.8],\n",
    "        'volume': [1000, 3000]\n",
    "    })\n",
    "    weekend_sla = pd.DataFrame({\n",
    "        'sla_rate': [0.8, 0.85],\n",
    "        'volume': [15000, 8000]\n",
    "    })\n",
    "    print(\"Using default temporal variables for feature engineering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f85a0d",
   "metadata": {},
   "source": [
    "**Key Performance Insights**\n",
    "- Weekend Performance Advantage: Saturday SLA achievement rates reached 92.7% compared to the worst weekday performance of 84.8% on Wednesday, creating a 7.9 percentage point daily performance gap, with weekend incidents totaling 340 cases versus 6,389 weekday incidents representing a 6.0% performance boost despite significantly lower volume.\n",
    "- Extreme Monthly Volatility: June achieved peak performance at 100.0% SLA while November recorded 0.0%, resulting in a 100.0 percentage point monthly performance range, with an overall declining trend of -37.8% per year indicating systematic performance degradation requiring immediate attention.\n",
    "- Q2-Q4 Seasonal Decline: Q2 demonstrated optimal performance at 90.8% SLA compared to Q4's 60.0%, creating a 30.8 percentage point quarterly gap with Q1 showing high variance (0.403) suggesting systematic seasonal resource constraint patterns.\n",
    "- Peak Hour Resource Strain: Peak business hours (9, 10, 8, 11, 14, 15) generate maximum incident volumes while achieving 86.0% average SLA compared to off-peak hours' 89.2%, resulting in a 3.1 percentage point performance penalty during high-volume periods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8666f6",
   "metadata": {},
   "source": [
    "#### (7) Location Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766658f6",
   "metadata": {},
   "source": [
    "This analysis evaluates geographic performance variations and infrastructure effectiveness to identify regional service delivery patterns and optimize location-based resource allocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb6c1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'location' in latest_incidents.columns:\n",
    "    try:\n",
    "        location_sla = latest_incidents.groupby('location').agg({\n",
    "            'sla_met': ['count', 'mean', 'std'],\n",
    "            'closed_time_hours':\n",
    "            'mean',\n",
    "            'reassignment_count':\n",
    "            'mean' if 'reassignment_count' in latest_incidents.columns else\n",
    "            lambda x: np.nan\n",
    "        }).round(3)\n",
    "\n",
    "        location_sla.columns = [\n",
    "            'incident_count', 'sla_compliance_rate', 'sla_std',\n",
    "            'avg_closed', 'avg_reassignments'\n",
    "        ]\n",
    "\n",
    "        # Filter locations with sufficient volume for reliable geographic performance comparison\n",
    "        # Location analysis requires minimum volume threshold due to geographic infrastructure dependencies\n",
    "        significant_locations = location_sla[location_sla['incident_count'] >= 20]\n",
    "\n",
    "        if len(significant_locations) > 0:\n",
    "            # Calculate risk scores for prioritization\n",
    "            significant_locations['risk_score'] = (\n",
    "                1 - significant_locations['sla_compliance_rate']\n",
    "            ) * significant_locations['incident_count']\n",
    "\n",
    "            plt.figure(figsize=(18, 10))\n",
    "\n",
    "            # 1. Top performing locations - Infrastructure best practices\n",
    "            plt.subplot(2, 2, 1)\n",
    "            top_locations = significant_locations.nlargest(\n",
    "                10, 'sla_compliance_rate')\n",
    "            colors = plt.cm.RdYlGn(top_locations['sla_compliance_rate'])\n",
    "            bars = plt.barh(range(len(top_locations)),\n",
    "                            top_locations['sla_compliance_rate'],\n",
    "                            color=colors)\n",
    "            plt.yticks(range(len(top_locations)),\n",
    "                       [name[:15] for name in top_locations.index],\n",
    "                       fontsize=8)\n",
    "            plt.xlabel('SLA Compliance Rate')\n",
    "            plt.title('Top 10 Performing Locations')\n",
    "\n",
    "            # Add volume info to bars\n",
    "            for i, (idx, data) in enumerate(top_locations.iterrows()):\n",
    "                plt.text(\n",
    "                    data['sla_compliance_rate'] + 0.01,\n",
    "                    i,\n",
    "                    f'{data[\"sla_compliance_rate\"]:.1%}\\n(n={int(data[\"incident_count\"])})',\n",
    "                    va='center',\n",
    "                    fontsize=7)\n",
    "\n",
    "            # 2. Highest risk locations - Priority intervention targets\n",
    "            plt.subplot(2, 2, 2)\n",
    "            high_risk_locations = significant_locations.nlargest(\n",
    "                10, 'risk_score')\n",
    "            bars = plt.barh(range(len(high_risk_locations)),\n",
    "                            high_risk_locations['risk_score'],\n",
    "                            color=plt.cm.Reds(\n",
    "                                np.linspace(0.3, 0.9,\n",
    "                                            len(high_risk_locations))))\n",
    "            plt.yticks(range(len(high_risk_locations)),\n",
    "                       [name[:15] for name in high_risk_locations.index],\n",
    "                       fontsize=8)\n",
    "            plt.xlabel('Risk Score (Breach Rate  Volume)')\n",
    "            plt.title('Highest Risk Locations')\n",
    "\n",
    "            # Add volume and SLA info to bars\n",
    "            for i, (loc, data) in enumerate(high_risk_locations.iterrows()):\n",
    "                plt.text(\n",
    "                    data['risk_score'] + 5,\n",
    "                    i,\n",
    "                    f\"SLA:{data['sla_compliance_rate']:.0%}\\n(n={int(data['incident_count'])})\",\n",
    "                    va='center',\n",
    "                    fontsize=7,\n",
    "                    color='darkred')\n",
    "\n",
    "            # 3. Volume vs SLA performance scatter - Resource allocation insights\n",
    "            plt.subplot(2, 2, 3)\n",
    "            scatter = plt.scatter(\n",
    "                significant_locations['incident_count'],\n",
    "                significant_locations['sla_compliance_rate'],\n",
    "                s=significant_locations['avg_reassignments'] *\n",
    "                30,  # Smaller points\n",
    "                alpha=0.7,\n",
    "                c=significant_locations['avg_closed'],\n",
    "                cmap='viridis')\n",
    "            plt.xlabel('Incident Volume')\n",
    "            plt.ylabel('SLA Compliance Rate')\n",
    "            plt.title(\n",
    "                'Volume vs SLA (Size=Reassignments, Color=Closed Time)')\n",
    "            plt.colorbar(scatter, label='Avg Closed (Hours)')\n",
    "\n",
    "            # Add trend line\n",
    "            if len(significant_locations) > 3:\n",
    "                z = np.polyfit(significant_locations['incident_count'],\n",
    "                               significant_locations['sla_compliance_rate'], 1)\n",
    "                p = np.poly1d(z)\n",
    "                plt.plot(significant_locations['incident_count'],\n",
    "                         p(significant_locations['incident_count']),\n",
    "                         \"--\",\n",
    "                         alpha=0.8,\n",
    "                         color='red')\n",
    "\n",
    "            # 4. Performance distribution with consistency\n",
    "            plt.subplot(2, 2, 4)\n",
    "\n",
    "            # Create performance tiers\n",
    "            high_perf = significant_locations[\n",
    "                significant_locations['sla_compliance_rate'] >= 0.85]\n",
    "            med_perf = significant_locations[\n",
    "                (significant_locations['sla_compliance_rate'] >= 0.7)\n",
    "                & (significant_locations['sla_compliance_rate'] < 0.85)]\n",
    "            low_perf = significant_locations[\n",
    "                significant_locations['sla_compliance_rate'] < 0.7]\n",
    "\n",
    "            # Create stacked bar chart\n",
    "            tier_data = {\n",
    "                'High (85%)': len(high_perf),\n",
    "                'Medium (70-85%)': len(med_perf),\n",
    "                'Low (<70%)': len(low_perf)\n",
    "            }\n",
    "\n",
    "            colors = ['green', 'orange', 'red']\n",
    "            bars = plt.bar(range(len(tier_data)),\n",
    "                           list(tier_data.values()),\n",
    "                           color=colors,\n",
    "                           alpha=0.7)\n",
    "            plt.xticks(range(len(tier_data)),\n",
    "                       list(tier_data.keys()),\n",
    "                       rotation=45)\n",
    "            plt.ylabel('Number of Locations')\n",
    "            plt.title('Location Performance Distribution')\n",
    "\n",
    "            # Add percentages on bars\n",
    "            total_locations = len(significant_locations)\n",
    "            for i, (tier, count) in enumerate(tier_data.items()):\n",
    "                pct = (count / total_locations) * 100\n",
    "                plt.text(i,\n",
    "                         count + 0.5,\n",
    "                         f'{count}\\n({pct:.1f}%)',\n",
    "                         ha='center',\n",
    "                         va='bottom',\n",
    "                         fontsize=9)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            # Detailed Location Analysis\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(\"DEATAILED ANALYSIS\")\n",
    "            print(f\"{'='*60}\")\n",
    "\n",
    "            # Overall statistics\n",
    "            print(f\"\\nOverall Location Performance Statistics:\")\n",
    "            print(f\"- Total locations analyzed: {len(significant_locations)}\")\n",
    "            print(f\"- Average SLA compliance: {significant_locations['sla_compliance_rate'].mean():.1%}\")\n",
    "            print(f\"- Performance range: {significant_locations['sla_compliance_rate'].min():.1%} - {significant_locations['sla_compliance_rate'].max():.1%}\")\n",
    "            print(f\"- Performance gap: {significant_locations['sla_compliance_rate'].max() - significant_locations['sla_compliance_rate'].min():.1%}\")\n",
    "\n",
    "            # Performance tier analysis\n",
    "            print(f\"\\nPerformance Tier Distribution:\")\n",
    "            print(f\"- High performing (85%): {len(high_perf)} locations ({len(high_perf)/len(significant_locations):.1%})\")\n",
    "            print(f\"- Medium performing (70-85%): {len(med_perf)} locations ({len(med_perf)/len(significant_locations):.1%})\")\n",
    "            print(f\"- Low performing (<70%): {len(low_perf)} locations ({len(low_perf)/len(significant_locations):.1%})\")\n",
    "\n",
    "            if len(high_perf) > 0:\n",
    "                print(f\"- High tier avg incidents: {high_perf['incident_count'].mean():.0f}\")\n",
    "            if len(low_perf) > 0:\n",
    "                print(f\"- Low tier avg incidents: {low_perf['incident_count'].mean():.0f}\")\n",
    "\n",
    "            # Volume concentration analysis\n",
    "            print(f\"\\nVolume Concentration Analysis:\")\n",
    "            total_incidents = significant_locations['incident_count'].sum()\n",
    "            top_20pct_threshold = significant_locations['incident_count'].quantile(0.8)\n",
    "            top_20pct_locations = significant_locations[\n",
    "                significant_locations['incident_count'] >= top_20pct_threshold\n",
    "            ]\n",
    "\n",
    "            print(f\"- Total incidents: {total_incidents:,}\")\n",
    "            print(f\"- Top 20% locations handle: {top_20pct_locations['incident_count'].sum():,} incidents ({top_20pct_locations['incident_count'].sum()/total_incidents:.1%})\")\n",
    "            print(f\"- Top 20% avg SLA: {top_20pct_locations['sla_compliance_rate'].mean():.1%}\")\n",
    "\n",
    "            # Risk assessment\n",
    "            critical_locations = significant_locations[\n",
    "                (significant_locations['sla_compliance_rate'] < 0.7) &\n",
    "                (significant_locations['incident_count'] > significant_locations['incident_count'].median())\n",
    "            ]\n",
    "\n",
    "            # Print summary tables\n",
    "            print(f\"\\nTop 5 Best Performing Locations:\")\n",
    "            print(\"Location | Incidents | SLA Rate | Avg Closed | Risk Score\")\n",
    "            print(\"-\" * 65)\n",
    "            for location, data in top_locations.head(5).iterrows():\n",
    "                location_name = str(location)[:12] + \"...\" if len(str(location)) > 15 else str(location)\n",
    "                print(f\"{location_name:12} | {data['incident_count']:9} | {data['sla_compliance_rate']:8.1%} | {data['avg_closed']:13.1f}h | {data['risk_score']:10.1f}\")\n",
    "\n",
    "            print(f\"\\nTop 5 Highest Risk Locations (Priority for Infrastructure Investment):\")\n",
    "            print(\"Location | Volume | SLA Rate | Avg Closed | Risk Score\")\n",
    "            print(\"-\" * 65)\n",
    "            for location, data in high_risk_locations.head(5).iterrows():\n",
    "                location_name = str(location)[:12] + \"...\" if len(str(location)) > 15 else str(location)\n",
    "                print(\n",
    "                    f\"{location_name:12} | {data['incident_count']:6} | {data['sla_compliance_rate']:8.1%} | {data['avg_closed']:13.1f}h | {data['risk_score']:10.1f}\"\n",
    "                )\n",
    "\n",
    "        else:\n",
    "            print(\"No locations with sufficient volume for analysis\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in location analysis: {e}\")\n",
    "else:\n",
    "    print(\"Location column not available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1abbe0",
   "metadata": {},
   "source": [
    "**Key Performance Insights**\n",
    "- Stable Geographic Performance: 32 locations show a 26.6% performance gap (72.7% to 99.3% SLA), with all locations maintaining above 70% threshold, indicating more consistent service delivery across geographic regions compared to organizational and process factors.\n",
    "- High-Capacity Center Strategy: Top 20% locations (7 locations) handle 83.1% of total incidents (5,131 cases) while maintaining 87.9% average SLA, demonstrating successful centralization of service delivery to major regional hubs without performance degradation.\n",
    "- Balanced Performance Distribution: 59.4% of locations achieve high performance (85% SLA) with 40.6% in medium tier (70-85%) and zero critical performers, suggesting effective geographic resource allocation across the location network.\n",
    "- Volume-Performance Relationship: High-tier locations average 225 incidents each, while top-performing locations handle varied volumes (26-280 incidents), indicating that volume alone does not determine geographic performance outcomes.\n",
    "- Risk Concentration Pattern: Highest risk locations (204: 179.7, 143: 142.1) maintain moderate SLA rates (87.7%, 83.2%) but generate high risk scores due to volume, suggesting risk stems from scale rather than fundamental service delivery failures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69076a15",
   "metadata": {},
   "source": [
    "### 4. Feature Engineering / Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d8fdde",
   "metadata": {},
   "source": [
    "This analysis implemented a systematic feature engineering approach, transforming 47 original variables into 76 candidate features before selecting 25 statistically robust predictors using Bonferroni correction and effect size thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abeea670",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"STEP 1: Feature Engineering\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a copy for processing\n",
    "df_processed = latest_incidents.copy()\n",
    "\n",
    "# 1. Time-based features (EDA validated)\n",
    "if 'opened_at' in df_processed.columns:\n",
    "    print(\"Creating time-based features...\")\n",
    "    df_processed['opened_hour'] = df_processed['opened_at'].dt.hour\n",
    "    df_processed['opened_dayofweek'] = df_processed['opened_at'].dt.dayofweek\n",
    "    df_processed['opened_month'] = df_processed['opened_at'].dt.month\n",
    "    df_processed['opened_quarter'] = df_processed['opened_at'].dt.quarter\n",
    "\n",
    "    # EDA validated patterns\n",
    "    df_processed['is_weekend'] = df_processed['opened_dayofweek'].isin(\n",
    "        [5, 6]).astype(int)\n",
    "    df_processed['is_business_hours'] = df_processed['opened_hour'].between(\n",
    "        9, 17).astype(int)\n",
    "    df_processed['is_q1_risk'] = (\n",
    "        df_processed['opened_quarter'] == 1).astype(int)\n",
    "    df_processed['is_peak_hour'] = df_processed['opened_hour'].isin(\n",
    "        [8, 9, 10, 11, 14, 15, 16]).astype(int)\n",
    "    df_processed['after_hours_advantage'] = (\n",
    "        df_processed['is_business_hours'] == 0).astype(int)\n",
    "\n",
    "    # Monthly patterns (discovered in EDA)\n",
    "    df_processed['problematic_month'] = df_processed['opened_month'].isin(\n",
    "        [10, 11]).astype(int)\n",
    "    df_processed['good_month'] = df_processed['opened_month'].isin(\n",
    "        [6,9]).astype(int)\n",
    "\n",
    "# 2. Boolean variable cleaning and improvement\n",
    "print(\"Processing boolean variables...\")\n",
    "boolean_mappings = {\n",
    "    'knowledge': 'knowledge_required',\n",
    "    'u_priority_confirmation': 'priority_confirmed',\n",
    "    'notify': 'notification_enabled'\n",
    "}\n",
    "\n",
    "for original_col, new_col in boolean_mappings.items():\n",
    "    if original_col in df_processed.columns:\n",
    "        df_processed[new_col] = df_processed[original_col].map({\n",
    "            True:\n",
    "            1,\n",
    "            'TRUE':\n",
    "            1,\n",
    "            't':\n",
    "            1,\n",
    "            'T':\n",
    "            1,\n",
    "            'Send Email':\n",
    "            1,\n",
    "            False:\n",
    "            0,\n",
    "            'FALSE':\n",
    "            0,\n",
    "            'f':\n",
    "            0,\n",
    "            'F':\n",
    "            0,\n",
    "            'Do Not Notify':\n",
    "            0\n",
    "        }).fillna(0).astype(int)\n",
    "\n",
    "# 3. Ordinal encoding of categorical variables (domain knowledge based)\n",
    "print(\"Creating ordinal encodings...\")\n",
    "ordinal_mappings = {\n",
    "    'priority': {\n",
    "        '1 - Critical': 1,\n",
    "        '2 - High': 2,\n",
    "        '3 - Moderate': 3,\n",
    "        '4 - Low': 4\n",
    "    },\n",
    "    'urgency': {\n",
    "        '1 - High': 1,\n",
    "        '2 - Medium': 2,\n",
    "        '3 - Low': 3\n",
    "    },\n",
    "    'impact': {\n",
    "        '1 - High': 1,\n",
    "        '2 - Medium': 2,\n",
    "        '3 - Low': 3\n",
    "    }\n",
    "}\n",
    "\n",
    "for col, mapping in ordinal_mappings.items():\n",
    "    if col in df_processed.columns:\n",
    "        df_processed[f'{col}_numeric'] = df_processed[col].map(mapping)\n",
    "\n",
    "# 4. Complexity indicators (EDA validated)\n",
    "if all(f'{col}_numeric' in df_processed.columns\n",
    "       for col in ['priority', 'impact', 'urgency']):\n",
    "    print(\"Creating complexity indicators...\")\n",
    "    df_processed['severity_score'] = (df_processed['priority_numeric'] +\n",
    "                                      df_processed['impact_numeric'] +\n",
    "                                      df_processed['urgency_numeric']) / 3\n",
    "\n",
    "    # High complexity case identification\n",
    "    df_processed['high_priority_case'] = (df_processed['priority_numeric']\n",
    "                                          <= 2).astype(int)\n",
    "    df_processed['high_impact_case'] = (df_processed['impact_numeric']\n",
    "                                        <= 2).astype(int)\n",
    "    df_processed['high_urgency_case'] = (df_processed['urgency_numeric']\n",
    "                                         <= 2).astype(int)\n",
    "\n",
    "    # Process consistency indicators\n",
    "    df_processed['priority_urgency_gap'] = abs(\n",
    "        df_processed['priority_numeric'] - df_processed['urgency_numeric'])\n",
    "    df_processed['has_priority_mismatch'] = (\n",
    "        df_processed['priority_urgency_gap'] > 1).astype(int)\n",
    "\n",
    "# 5. Interaction features (EDA based)\n",
    "print(\"Creating interaction features...\")\n",
    "if 'is_weekend' in df_processed.columns and 'priority_numeric' in df_processed.columns:\n",
    "    df_processed['weekend_priority_interaction'] = df_processed[\n",
    "        'is_weekend'] * df_processed['priority_numeric']\n",
    "\n",
    "if 'knowledge_required' in df_processed.columns and 'priority_numeric' in df_processed.columns:\n",
    "    df_processed['knowledge_complexity_case'] = (\n",
    "        df_processed['knowledge_required'] *\n",
    "        (df_processed['priority_numeric'] <= 2)).astype(int)\n",
    "\n",
    "# 6. Frequency encoding + Label encoding for categorical variables\n",
    "print(\"Processing categorical features...\")\n",
    "categorical_features = ['assignment_group', 'category', 'location']\n",
    "\n",
    "for feature in categorical_features:\n",
    "    if feature in df_processed.columns:\n",
    "        # Missing value handling\n",
    "        df_processed[feature] = df_processed[feature].fillna('Unknown')\n",
    "\n",
    "        # Frequency encoding\n",
    "        value_counts = df_processed[feature].value_counts()\n",
    "        df_processed[f'{feature}_frequency'] = df_processed[feature].map(\n",
    "            value_counts)\n",
    "        df_processed[f'{feature}_frequency_log'] = np.log1p(\n",
    "            df_processed[f'{feature}_frequency'])\n",
    "\n",
    "        # Label encoding\n",
    "        le = LabelEncoder()\n",
    "        df_processed[f'{feature}_encoded'] = le.fit_transform(\n",
    "            df_processed[feature].astype(str))\n",
    "\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 2: Statistical Feature Analysis (with Duplicate Removal)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define features for analysis\n",
    "engineered_features = [\n",
    "    'opened_hour', 'opened_dayofweek', 'opened_month', 'opened_quarter',\n",
    "    'is_weekend', 'is_business_hours', 'is_q1_risk', 'is_peak_hour',\n",
    "    'after_hours_advantage', 'problematic_month', 'good_month',\n",
    "    'knowledge_required', 'priority_confirmed', 'notification_enabled',\n",
    "    'priority_numeric', 'urgency_numeric', 'impact_numeric', 'severity_score',\n",
    "    'high_priority_case', 'high_impact_case', 'high_urgency_case',\n",
    "    'priority_urgency_gap', 'has_priority_mismatch',\n",
    "    'weekend_priority_interaction', 'knowledge_complexity_case',\n",
    "    'assignment_group_frequency_log', 'category_frequency_log',\n",
    "    'location_frequency_log', 'assignment_group_encoded', 'category_encoded',\n",
    "    'location_encoded'\n",
    "]\n",
    "\n",
    "original_features = [\n",
    "    'assignment_group', 'priority', 'category', 'subcategory', 'knowledge',\n",
    "    'u_priority_confirmation', 'urgency', 'location', 'impact', 'notify'\n",
    "]\n",
    "\n",
    "# Select only existing features\n",
    "raw_features = [\n",
    "    f for f in engineered_features + original_features\n",
    "    if f in df_processed.columns and f != 'sla_met'\n",
    "]\n",
    "\n",
    "# Remove duplicate features\n",
    "print(\"Removing duplicate features...\")\n",
    "replacements = {\n",
    "    'knowledge': 'knowledge_required',\n",
    "    'u_priority_confirmation': 'priority_confirmed',\n",
    "    'notify': 'notification_enabled',\n",
    "    'priority': 'priority_numeric',\n",
    "    'urgency': 'urgency_numeric',\n",
    "    'impact': 'impact_numeric'\n",
    "}\n",
    "\n",
    "features_to_remove = set()\n",
    "\n",
    "# Handle clear replacement relationships\n",
    "for original, replacement in replacements.items():\n",
    "    if original in raw_features and replacement in raw_features:\n",
    "        features_to_remove.add(original)\n",
    "\n",
    "# Detect features with identical values\n",
    "for i, feat1 in enumerate(raw_features):\n",
    "    for feat2 in raw_features[i + 1:]:\n",
    "        if feat1 in features_to_remove or feat2 in features_to_remove:\n",
    "            continue\n",
    "        if feat1 in df_processed.columns and feat2 in df_processed.columns:\n",
    "            try:\n",
    "                # Check for perfect numerical match\n",
    "                if (df_processed[feat1].dtype in ['int64', 'float64']\n",
    "                        and df_processed[feat2].dtype in ['int64', 'float64']\n",
    "                        and df_processed[feat1].equals(df_processed[feat2])):\n",
    "\n",
    "                    # Decide based on correlation with target\n",
    "                    corr1, _ = pearsonr(df_processed[feat1].fillna(0),\n",
    "                                        df_processed['sla_met'])\n",
    "                    corr2, _ = pearsonr(df_processed[feat2].fillna(0),\n",
    "                                        df_processed['sla_met'])\n",
    "\n",
    "                    if abs(corr1) >= abs(corr2):\n",
    "                        features_to_remove.add(feat2)\n",
    "                    else:\n",
    "                        features_to_remove.add(feat1)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "all_features = [f for f in raw_features if f not in features_to_remove]\n",
    "\n",
    "print(f\"  - Removed {len(features_to_remove)} duplicate features\")\n",
    "print(f\"  - Final count: {len(all_features)} features\")\n",
    "if features_to_remove:\n",
    "    print(\"  - Removed features:\", list(features_to_remove))\n",
    "\n",
    "print(f\"\\nAnalyzing {len(all_features)} features after duplicate removal...\")\n",
    "\n",
    "\n",
    "# Effect size interpretation functions\n",
    "def interpret_cramers_v(v):\n",
    "    if v < 0.1: return \"Very Small\"\n",
    "    elif v < 0.3: return \"Small\"\n",
    "    elif v < 0.5: return \"Medium\"\n",
    "    else: return \"Large\"\n",
    "\n",
    "\n",
    "def interpret_correlation(r):\n",
    "    if r < 0.1: return \"Very Small\"\n",
    "    elif r < 0.3: return \"Small\"\n",
    "    elif r < 0.5: return \"Medium\"\n",
    "    else: return \"Large\"\n",
    "\n",
    "\n",
    "# Execute statistical analysis\n",
    "results = []\n",
    "target_col = 'sla_met'\n",
    "alpha = 0.05\n",
    "\n",
    "for feature in all_features:\n",
    "    try:\n",
    "        df_clean = df_processed[[feature, target_col]].dropna()\n",
    "        if len(df_clean) == 0 or df_clean[feature].nunique() <= 1:\n",
    "            continue\n",
    "\n",
    "        if df_processed[feature].dtype in ['object', 'category']:\n",
    "            # Categorical: Chi-square test\n",
    "            contingency_table = pd.crosstab(df_clean[feature],\n",
    "                                            df_clean[target_col])\n",
    "            chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "            n = contingency_table.sum().sum()\n",
    "            cramers_v = np.sqrt(chi2 / (n *\n",
    "                                        (min(contingency_table.shape) - 1)))\n",
    "\n",
    "            results.append({\n",
    "                'Feature':\n",
    "                feature,\n",
    "                'Test_Type':\n",
    "                'Chi-square',\n",
    "                'Statistic':\n",
    "                chi2,\n",
    "                'P_Value':\n",
    "                p_value,\n",
    "                'Effect_Size':\n",
    "                cramers_v,\n",
    "                'Effect_Interpretation':\n",
    "                interpret_cramers_v(cramers_v),\n",
    "                'Sample_Size':\n",
    "                n,\n",
    "                'Significant':\n",
    "                p_value < alpha,\n",
    "                'Feature_Type':\n",
    "                'Engineered' if feature in engineered_features else 'Original'\n",
    "            })\n",
    "        else:\n",
    "            # Numerical: Correlation analysis\n",
    "            corr, p_value = pearsonr(df_clean[feature], df_clean[target_col])\n",
    "\n",
    "            results.append({\n",
    "                'Feature':\n",
    "                feature,\n",
    "                'Test_Type':\n",
    "                'Correlation',\n",
    "                'Statistic':\n",
    "                abs(corr),\n",
    "                'P_Value':\n",
    "                p_value,\n",
    "                'Effect_Size':\n",
    "                abs(corr),\n",
    "                'Effect_Interpretation':\n",
    "                interpret_correlation(abs(corr)),\n",
    "                'Sample_Size':\n",
    "                len(df_clean),\n",
    "                'Significant':\n",
    "                p_value < alpha,\n",
    "                'Feature_Type':\n",
    "                'Engineered' if feature in engineered_features else 'Original'\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {feature}: {e}\")\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Bonferroni multiple comparison correction\n",
    "if len(results_df) > 0:\n",
    "    n_tests = len(results_df)\n",
    "    corrected_alpha = alpha / n_tests\n",
    "    results_df['Bonferroni_Significant'] = results_df[\n",
    "        'P_Value'] < corrected_alpha\n",
    "\n",
    "    print(f\"\\nStatistical Analysis Summary:\")\n",
    "    print(f\"  - Total tests: {n_tests}\")\n",
    "    print(f\"  - Significant (original): {results_df['Significant'].sum()}\")\n",
    "    print(\n",
    "        f\"  - Significant (Bonferroni): {results_df['Bonferroni_Significant'].sum()}\"\n",
    "    )\n",
    "\n",
    "results_df = results_df.sort_values('Effect_Size', ascending=False)\n",
    "\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 3: Final Feature Selection\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Selection parameters\n",
    "use_bonferroni = True  # Apply Bonferroni correction for multiple testing\n",
    "min_effect_size = 0.02  # Minimum effect size threshold (Cohen's convention)\n",
    "\n",
    "# Choose significance criterion\n",
    "significance_col = 'Bonferroni_Significant' if use_bonferroni else 'Significant'\n",
    "\n",
    "# Select statistically significant features with sufficient effect size\n",
    "selected_features = results_df[(results_df[significance_col] == True) & (\n",
    "    results_df['Effect_Size'] >= min_effect_size)]['Feature'].tolist()\n",
    "\n",
    "print(f\"Selection Criteria:\")\n",
    "print(f\"  - Use Bonferroni correction: {use_bonferroni}\")\n",
    "print(f\"  - Minimum effect size: {min_effect_size}\")\n",
    "print(f\"  - Selected features: {len(selected_features)}\")\n",
    "\n",
    "if len(selected_features) > 0:\n",
    "    print(f\"\\nSelected Features:\")\n",
    "    selected_results = results_df[results_df['Feature'].isin(\n",
    "        selected_features)]\n",
    "    for _, row in selected_results.iterrows():\n",
    "        print(f\"  {row['Feature']:30} | {row['Feature_Type']:10} | \"\n",
    "              f\"Effect: {row['Effect_Size']:.3f} | p: {row['P_Value']:.4f}\")\n",
    "else:\n",
    "    print(\"No features met the selection criteria. Using relaxed criteria...\")\n",
    "    # Reselect with relaxed criteria\n",
    "    relaxed_features = results_df[(results_df['Significant'] == True) & (\n",
    "        results_df['Effect_Size'] >= 0.01)]['Feature'].head(15).tolist()\n",
    "\n",
    "    if relaxed_features:\n",
    "        print(\n",
    "            f\"Using relaxed criteria, selected top {len(relaxed_features)} features:\"\n",
    "        )\n",
    "        for feature in relaxed_features:\n",
    "            print(f\"  - {feature}\")\n",
    "        selected_features = relaxed_features\n",
    "\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 4: Key Results Visualization\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# 1. Top 15 feature importance\n",
    "if len(results_df) > 0:\n",
    "    top_15 = results_df.head(15)\n",
    "    colors = [\n",
    "        'darkgreen' if ft == 'Engineered' else 'lightblue'\n",
    "        for ft in top_15['Feature_Type']\n",
    "    ]\n",
    "\n",
    "    axes[0].barh(range(len(top_15)),\n",
    "                 top_15['Effect_Size'],\n",
    "                 color=colors,\n",
    "                 alpha=0.8)\n",
    "    axes[0].set_yticks(range(len(top_15)))\n",
    "    axes[0].set_yticklabels([f[:25] for f in top_15['Feature']], fontsize=9)\n",
    "    axes[0].set_xlabel('Effect Size')\n",
    "    axes[0].set_title(\n",
    "        'Top 15 Most Important Features\\n(Green=Engineered, Blue=Original)')\n",
    "\n",
    "    for i, (feature,\n",
    "            effect) in enumerate(zip(top_15['Feature'],\n",
    "                                     top_15['Effect_Size'])):\n",
    "        axes[0].text(effect + 0.005,\n",
    "                     i,\n",
    "                     f'{effect:.3f}',\n",
    "                     va='center',\n",
    "                     fontsize=8,\n",
    "                     fontweight='bold')\n",
    "\n",
    "# 2. EDA validated key patterns\n",
    "key_eda_features = [\n",
    "    'is_weekend', 'is_q1_risk', 'priority_numeric', 'knowledge_required'\n",
    "]\n",
    "available_eda_features = [\n",
    "    f for f in key_eda_features if f in df_processed.columns\n",
    "]\n",
    "\n",
    "if available_eda_features:\n",
    "    axes[1].set_title('EDA-Validated Key Patterns')\n",
    "    y_pos = 0\n",
    "    colors_eda = ['lightgreen', 'orange', 'lightcoral', 'lightblue']\n",
    "\n",
    "    for i, feature in enumerate(available_eda_features[:4]):\n",
    "        if df_processed[feature].nunique() == 2:  # Binary variable\n",
    "            stats = df_processed.groupby(feature)[target_col].mean()\n",
    "            if len(stats) == 2:\n",
    "                effect = abs(stats.iloc[1] - stats.iloc[0])\n",
    "                axes[1].barh(y_pos,\n",
    "                             effect,\n",
    "                             color=colors_eda[i],\n",
    "                             alpha=0.7,\n",
    "                             height=0.6)\n",
    "                axes[1].text(effect + 0.01,\n",
    "                             y_pos,\n",
    "                             f'{feature}\\n{effect:.1%} gap',\n",
    "                             va='center',\n",
    "                             fontsize=9)\n",
    "                y_pos += 1\n",
    "        elif feature == 'priority_numeric':\n",
    "            stats = df_processed.groupby('priority')[target_col].mean()\n",
    "            if len(stats) > 1:\n",
    "                effect = stats.max() - stats.min()\n",
    "                axes[1].barh(y_pos,\n",
    "                             effect,\n",
    "                             color=colors_eda[i],\n",
    "                             alpha=0.7,\n",
    "                             height=0.6)\n",
    "                axes[1].text(effect + 0.01,\n",
    "                             y_pos,\n",
    "                             f'{feature}\\n{effect:.1%} range',\n",
    "                             va='center',\n",
    "                             fontsize=9)\n",
    "                y_pos += 1\n",
    "\n",
    "    axes[1].set_xlabel('Performance Gap')\n",
    "    axes[1].set_yticks([])\n",
    "    axes[1].set_xlim(0, max(0.5, axes[1].get_xlim()[1] * 1.2))\n",
    "\n",
    "# 3. Feature selection summary\n",
    "if len(results_df) > 0:\n",
    "    selection_summary = {\n",
    "        'Original':\n",
    "        len([\n",
    "            f for f in selected_features if f in results_df[\n",
    "                results_df['Feature_Type'] == 'Original']['Feature'].values\n",
    "        ]),\n",
    "        'Engineered':\n",
    "        len([\n",
    "            f for f in selected_features if f in results_df[\n",
    "                results_df['Feature_Type'] == 'Engineered']['Feature'].values\n",
    "        ])\n",
    "    }\n",
    "\n",
    "    total_original = len(results_df[results_df['Feature_Type'] == 'Original'])\n",
    "    total_engineered = len(\n",
    "        results_df[results_df['Feature_Type'] == 'Engineered'])\n",
    "\n",
    "    categories = ['Original\\nFeatures', 'Engineered\\nFeatures']\n",
    "    totals = [total_original, total_engineered]\n",
    "    selected_counts = [\n",
    "        selection_summary['Original'], selection_summary['Engineered']\n",
    "    ]\n",
    "\n",
    "    x = range(len(categories))\n",
    "    width = 0.35\n",
    "\n",
    "    axes[2].bar([i - width / 2 for i in x],\n",
    "                totals,\n",
    "                width,\n",
    "                label='Total Available',\n",
    "                alpha=0.7,\n",
    "                color='lightgray')\n",
    "    axes[2].bar([i + width / 2 for i in x],\n",
    "                selected_counts,\n",
    "                width,\n",
    "                label='Selected',\n",
    "                alpha=0.8,\n",
    "                color=['lightblue', 'darkgreen'])\n",
    "\n",
    "    axes[2].set_xticks(x)\n",
    "    axes[2].set_xticklabels(categories)\n",
    "    axes[2].set_ylabel('Feature Count')\n",
    "    axes[2].set_title('Feature Selection Summary')\n",
    "    axes[2].legend()\n",
    "\n",
    "    for i, (total, sel) in enumerate(zip(totals, selected_counts)):\n",
    "        if total > 0:\n",
    "            rate = sel / total\n",
    "            axes[2].text(i,\n",
    "                         max(total, sel) + 1,\n",
    "                         f'{rate:.1%}\\nselected',\n",
    "                         ha='center',\n",
    "                         va='bottom',\n",
    "                         fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 5: Feature Correlation Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Correlation Matrix for Selected Features\n",
    "numerical_features = [\n",
    "    f for f in selected_features\n",
    "    if df_processed[f].dtype in ['int64', 'float64']\n",
    "]\n",
    "\n",
    "print(\n",
    "    f\"Analyzing correlations among {len(numerical_features)} numerical features...\"\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(14, 12))\n",
    "correlation_matrix = df_processed[numerical_features].corr()\n",
    "sns.heatmap(correlation_matrix,\n",
    "            annot=True,\n",
    "            cmap='coolwarm',\n",
    "            center=0,\n",
    "            fmt='.2f',\n",
    "            square=True)\n",
    "plt.title('Selected Features Correlation Matrix')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nHigh correlation pairs (|r| > 0.7):\")\n",
    "high_corr = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i + 1, len(correlation_matrix.columns)):\n",
    "        corr_val = correlation_matrix.iloc[i, j]\n",
    "        if abs(corr_val) > 0.7:\n",
    "            high_corr.append(\n",
    "                f\"  {correlation_matrix.columns[i]}  {correlation_matrix.columns[j]}: {corr_val:.3f}\"\n",
    "            )\n",
    "\n",
    "if high_corr:\n",
    "    for pair in high_corr:\n",
    "        print(pair)\n",
    "else:\n",
    "    print(\"  No highly correlated pairs found (good for model stability)\")\n",
    "\n",
    "\n",
    "# Final results\n",
    "print(f\"\\nFINAL RESULTS:\")\n",
    "print(f\"  - Original features: {latest_incidents.shape[1]}\")\n",
    "print(f\"  - After engineering: {df_processed.shape[1]}\")\n",
    "print(f\"  - Selected features: {len(selected_features)}\")\n",
    "print(\n",
    "    f\"  - Final dataset shape: ({df_processed.shape[0]}, {len(selected_features)})\"\n",
    ")\n",
    "\n",
    "if selected_features:\n",
    "    X_final = df_processed[selected_features]\n",
    "    y_final = df_processed['sla_met']\n",
    "\n",
    "    print(f\"\\nReady for modeling!\")\n",
    "    print(f\"X_final.shape: {X_final.shape}\")\n",
    "    print(f\"y_final.shape: {y_final.shape}\")\n",
    "else:\n",
    "    print(f\"\\nNo features selected. Please review the criteria.\")\n",
    "\n",
    "# Check results\n",
    "print(\"\\nSelected features:\", selected_features)\n",
    "print(\"\\nTop 10 features by effect size:\")\n",
    "print(results_df[['Feature', 'Effect_Size', 'Feature_Type']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a2ce6f",
   "metadata": {},
   "source": [
    "**Key Insights**\n",
    "- Statistical Rigor: 25 of 34 features survived Bonferroni correction (p < 0.00147), demonstrating strong statistical validity. The balance of 4 original features (100% selection rate) and 21 engineered features (84% selection rate) indicates successful domain-driven feature creation.\n",
    "- EDA Pattern Validation: Key discoveries from exploratory analysis were successfully captured as features - Q1 risk (effect: 0.156), priority paradox (high_priority_case: 0.167), and weekend effects (weekend_priority_interaction: 0.040) all achieved significance.\n",
    "- Multicollinearity Concerns: Correlation analysis revealed severe dependencies among priority-related features, with severity_score showing 0.98+ correlation with individual components and perfect negative correlation between is_business_hours and after_hours_advantage (-1.000), potentially leading to unstable model predictions and inflated feature importance for priority-related variables.\n",
    "- Multi-dimensional Performance Drivers: Top features reveal balanced importance across multiple domains - organizational factors (assignment_group: 0.294, location: 0.199), service categorization (subcategory: 0.279, category: 0.183), process indicators (priority_confirmed: 0.166), and complexity measures (high_priority_case: 0.167), indicating that SLA performance is driven by the combination of \"who handles it,\" \"what type of issue it is,\" and \"how it's processed\" rather than any single factor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c98be6",
   "metadata": {},
   "source": [
    "### 5. Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fae907",
   "metadata": {},
   "source": [
    "**Research Questions**\n",
    "\n",
    "This data analysis was designed to answer the key question: **\"What factors have the biggest impact on IT incident SLA performance?\"** \n",
    "\n",
    "The results show that assignment_group (effect size 0.294) and subcategory (effect size 0.279) emerged as the top predictors with nearly equal importance. This indicates that both \"who handles the incident\" and \"what type of issue it is\" are critical factors, with organizational and service classification factors showing comparable impact on SLA performance.\n",
    "\n",
    "**Analysis Process Evaluation**\n",
    "\n",
    "I completed all the required steps for this assignment:\n",
    "- **Data cleaning**: Removed negative closed times, converted dates properly\n",
    "- **Missing data handling**: Dropped columns with 95%+ missing values, used 'Unknown' category for categorical variables\n",
    "- **Outlier treatment**: Kept extreme values since they represent real business scenarios\n",
    "- **EDA**: Used Chi-square and Pearson correlation to explore feature relationships\n",
    "- **Correlation matrix**: Identified multicollinearity issues\n",
    "\n",
    "**Problems I Found**\n",
    "\n",
    "1. **Multicollinearity**: The severity_score has 98% correlation with priority_numeric - they're basically the same information.\n",
    "2. **Data characteristics**: Average closed time of 125 days suggests these aren't typical IT incidents but major projects.\n",
    "3. **Generalization limits**: These results probably won't apply directly to other organizations.\n",
    "\n",
    "**Practical Applications**\n",
    "\n",
    "If I were to build a real prediction model with this data:\n",
    "- **Random Forest**: Handles mixed data types well and is easy to interpret\n",
    "- **XGBoost**: Would probably give the best performance\n",
    "- **Need regularization techniques** because of the multicollinearity problem\n",
    "\n",
    "**Limitations and Improvements**\n",
    "This dataset is quite different from typical IT helpdesk data. The closed times are too long - it looks more like change management or project data than actual incidents. In practice, we'd need to reanalyze with shorter timeframe data to get more realistic results for day-to-day IT operations.\n",
    "The priority system also seems broken - critical incidents only have 53% SLA compliance while low priority ones achieve 95%. This suggests either unrealistic SLA targets for urgent issues or serious resource allocation problems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp647_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

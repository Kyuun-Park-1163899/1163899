{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75d94c43",
   "metadata": {},
   "source": [
    "# IT Incident SLA Comliance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df9e9a1",
   "metadata": {},
   "source": [
    "Service Level Agreement (SLA) compliance represents a critical performance metric in IT service management, directly impacting customer satisfaction, operational efficiency, and business continuity. This analysis examines cleaned IT incident data to identify systematic patterns affecting SLA compliance rates and builds predictive models to forecast potential SLA breaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "537599eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6729 entries, 0 to 6728\n",
      "Data columns (total 32 columns):\n",
      " #   Column                   Non-Null Count  Dtype \n",
      "---  ------                   --------------  ----- \n",
      " 0   number                   6729 non-null   object\n",
      " 1   incident_state           6729 non-null   object\n",
      " 2   active                   6729 non-null   bool  \n",
      " 3   reassignment_count       6729 non-null   int64 \n",
      " 4   reopen_count             6729 non-null   int64 \n",
      " 5   sys_mod_count            6729 non-null   int64 \n",
      " 6   made_sla                 6729 non-null   bool  \n",
      " 7   caller_id                6727 non-null   object\n",
      " 8   opened_by                6432 non-null   object\n",
      " 9   opened_at                6729 non-null   object\n",
      " 10  sys_created_by           3784 non-null   object\n",
      " 11  sys_created_at           3784 non-null   object\n",
      " 12  sys_updated_by           6729 non-null   object\n",
      " 13  sys_updated_at           6729 non-null   object\n",
      " 14  contact_type             6729 non-null   object\n",
      " 15  location                 6724 non-null   object\n",
      " 16  category                 6724 non-null   object\n",
      " 17  subcategory              6723 non-null   object\n",
      " 18  symptom                  5137 non-null   object\n",
      " 19  impact                   6729 non-null   object\n",
      " 20  urgency                  6729 non-null   object\n",
      " 21  priority                 6729 non-null   object\n",
      " 22  assignment_group         6034 non-null   object\n",
      " 23  assigned_to              6410 non-null   object\n",
      " 24  knowledge                6729 non-null   bool  \n",
      " 25  u_priority_confirmation  6729 non-null   bool  \n",
      " 26  notify                   6729 non-null   object\n",
      " 27  closed_code              6705 non-null   object\n",
      " 28  resolved_by              6701 non-null   object\n",
      " 29  resolved_at              6291 non-null   object\n",
      " 30  closed_at                6729 non-null   object\n",
      " 31  isParent                 6729 non-null   object\n",
      "dtypes: bool(4), int64(3), object(25)\n",
      "memory usage: 1.5+ MB\n",
      "\n",
      "Dataset shape: (6729, 32)\n",
      "\n",
      "Target Variable Distribution:\n",
      "made_sla\n",
      "True     5813\n",
      "False     916\n",
      "Name: count, dtype: int64\n",
      "SLA Compliance Rate: 86.4%\n",
      "SLA Breach Rate: 13.6%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import warnings\n",
    "import shap\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns',\n",
    "              None)  # Display all columns in DataFrame output.\n",
    "pd.set_option('display.max_rows',\n",
    "              None)  # Display all rows in DataFrame output.\n",
    "# Load data from dataset\n",
    "df = pd.read_csv('../data/incidents_cleaned.csv')\n",
    "\n",
    "# Display DataFrame information\n",
    "df.info()\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"\\nTarget Variable Distribution:\")\n",
    "print(df['made_sla'].value_counts())\n",
    "print(f\"SLA Compliance Rate: {df['made_sla'].mean():.1%}\")\n",
    "print(f\"SLA Breach Rate: {(1 - df['made_sla'].mean()):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd46cfc",
   "metadata": {},
   "source": [
    "## 1. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab036cb",
   "metadata": {},
   "source": [
    "### 1.1 Operational Complexity Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "748ae9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fe = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ac0a0a",
   "metadata": {},
   "source": [
    "Rationale: EDA revealed sys_mod_count (r=-0.37) and reassignment_count (r=-0.25) as top predictors. Raw counts don't capture intensity (speed of handling), so we create ratio features normalized by incident duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "54299497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created 3 operational complexity features:\n",
      "  - activity_score: mods + 2×reassignments (reopen excluded)\n",
      "  - has_reassignment, high_modification (binary flags)\n",
      "  - is_complex: >4 mods OR >2 reassignments\n",
      "\n",
      "Validation - is_complex feature:\n",
      "  Simple: 98.7%, Complex: 69.4%\n",
      "  Gap: 29.2%\n"
     ]
    }
   ],
   "source": [
    "# Raw operational counts (available during incident)\n",
    "# sys_mod_count: System modifications made so far\n",
    "# reassignment_count: Number of reassignments that occurred\n",
    "\n",
    "# Weighted activity score (MODIFIED: exclude reopen_count)\n",
    "# Weights: modifications (1x), reassignments (2x)\n",
    "df_fe['activity_score'] = (df_fe['sys_mod_count'] +\n",
    "                           df_fe['reassignment_count'] * 2)\n",
    "\n",
    "# Binary complexity flags\n",
    "df_fe['has_reassignment'] = (df_fe['reassignment_count'] > 0).astype(int)\n",
    "df_fe['high_modification'] = (df_fe['sys_mod_count'] > 3).astype(int)\n",
    "\n",
    "# Simplified complexity flag (MODIFIED: exclude reopen_count condition)\n",
    "df_fe['is_complex'] = ((df_fe['sys_mod_count'] > 4) |\n",
    "                       (df_fe['reassignment_count'] > 2)).astype(int)\n",
    "\n",
    "print(\"✓ Created 3 operational complexity features:\")\n",
    "print(\"  - activity_score: mods + 2×reassignments (reopen excluded)\")\n",
    "print(\"  - has_reassignment, high_modification (binary flags)\")\n",
    "print(\"  - is_complex: >4 mods OR >2 reassignments\")\n",
    "\n",
    "# Validation\n",
    "complex_sla = df_fe.groupby('is_complex')['made_sla'].mean()\n",
    "print(f\"\\nValidation - is_complex feature:\")\n",
    "print(f\"  Simple: {complex_sla[0]:.1%}, Complex: {complex_sla[1]:.1%}\")\n",
    "print(f\"  Gap: {(complex_sla[0]-complex_sla[1])*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567d3905",
   "metadata": {},
   "source": [
    "### 1.2 Severity Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f04eea9",
   "metadata": {},
   "source": [
    "Rationale: Priority, Impact, and Urgency show strong multicollinearity (r=0.75-0.89), indicating they capture overlapping severity information. Using all three would introduce redundancy. We create binary \"high severity\" flags instead. EDA showed Priority has widest performance gap (Critical: 53.3% vs Low: 95.5% = 42.2% gap)From EDA: Priority shows 42.2% gap, but high multicollinearity with impact/urgency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ddccb9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created 4 severity-based features\n",
      "Validation: Low 87.4%, High 54.9%, Gap 32.5%\n"
     ]
    }
   ],
   "source": [
    "# Binary severity flags (convert categorical to 0/1)\n",
    "df_fe['is_high_priority'] = df_fe['priority'].isin(\n",
    "    ['1 - Critical', '2 - High']).astype(int)\n",
    "df_fe['is_high_impact'] = df_fe['impact'].isin(['1 - High']).astype(int)\n",
    "df_fe['is_high_urgency'] = df_fe['urgency'].isin(['1 - High']).astype(int)\n",
    "df_fe['is_high_severity'] = ((df_fe['is_high_priority'] == 1) |\n",
    "                             (df_fe['is_high_impact'] == 1) |\n",
    "                             (df_fe['is_high_urgency'] == 1)).astype(int)\n",
    "\n",
    "print(\"✓ Created 4 severity-based features\")\n",
    "\n",
    "sev_sla = df_fe.groupby('is_high_severity')['made_sla'].mean()\n",
    "print(\n",
    "    f\"Validation: Low {sev_sla[0]:.1%}, High {sev_sla[1]:.1%}, Gap {(sev_sla[0]-sev_sla[1])*100:.1f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874f059e",
   "metadata": {},
   "source": [
    "### 1.3 Interaction Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c55a72c",
   "metadata": {},
   "source": [
    "Rationale: Combinations of factors may have compounding effects on SLA compliance. High severity + High complexity likely exhibits multiplicative negative impact rather than simple additive effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "170d4013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created 3 interaction features\n",
      "Validation: No risk 87.3%, High risk 50.3%, Gap 36.9%\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Feature Engineering Summary:\n",
      "  Original: 32 → Engineered: 43 (+11 features)\n",
      "  Excluded post-incident metrics: closed_time, reopen_count, per_day ratios\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Severity + Complexity\n",
    "df_fe['severity_complexity'] = (df_fe['is_high_severity'] * df_fe['is_complex']).astype(int)\n",
    "\n",
    "# Priority confirmation + Complexity  \n",
    "# Note: u_priority_confirmation is available at incident time (procedural flag)\n",
    "df_fe['confirmed_complex'] = (df_fe['u_priority_confirmation'] * df_fe['is_complex']).astype(int)\n",
    "\n",
    "# High priority + Reassignment\n",
    "df_fe['priority_reassign'] = (df_fe['is_high_priority'] * df_fe['has_reassignment']).astype(int)\n",
    "\n",
    "print(\"✓ Created 3 interaction features\")\n",
    "\n",
    "risk_sla = df_fe.groupby('severity_complexity')['made_sla'].mean()\n",
    "print(f\"Validation: No risk {risk_sla[0]:.1%}, High risk {risk_sla[1]:.1%}, Gap {(risk_sla[0]-risk_sla[1])*100:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(f\"Feature Engineering Summary:\")\n",
    "print(f\"  Original: {df.shape[1]} → Engineered: {df_fe.shape[1]} (+{df_fe.shape[1]-df.shape[1]} features)\")\n",
    "print(f\"  Excluded post-incident metrics: closed_time, reopen_count, per_day ratios\")\n",
    "print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091c0c95",
   "metadata": {},
   "source": [
    "### 1.4 Feature Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289c2770",
   "metadata": {},
   "source": [
    "#### 1.4.1 Ordinal Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ff6400",
   "metadata": {},
   "source": [
    "Priority, Impact, and Urgency have natural ordering from low to high severity. Preserve this ordering with numeric encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eff07bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enc = df_fe.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a297f08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Ordinal encoded 3 variables:\n",
      "  - priority_enc: 1 (Low) → 4 (Critical)\n",
      "  - impact_enc: 1 (Low) → 3 (High)\n",
      "  - urgency_enc: 1 (Low) → 3 (High)\n"
     ]
    }
   ],
   "source": [
    "# Priority (1=Low, 4=Critical)\n",
    "priority_map = {\n",
    "    '4 - Low': 1,\n",
    "    '3 - Moderate': 2,\n",
    "    '2 - High': 3,\n",
    "    '1 - Critical': 4\n",
    "}\n",
    "df_enc['priority_enc'] = df_enc['priority'].map(priority_map)\n",
    "\n",
    "# Impact (1=Low, 3=High)\n",
    "impact_map = {'3 - Low': 1, '2 - Medium': 2, '1 - High': 3}\n",
    "df_enc['impact_enc'] = df_enc['impact'].map(impact_map)\n",
    "\n",
    "# Urgency (1=Low, 3=High)\n",
    "urgency_map = {'3 - Low': 1, '2 - Medium': 2, '1 - High': 3}\n",
    "df_enc['urgency_enc'] = df_enc['urgency'].map(urgency_map)\n",
    "\n",
    "print(\"✓ Ordinal encoded 3 variables:\")\n",
    "print(\"  - priority_enc: 1 (Low) → 4 (Critical)\")\n",
    "print(\"  - impact_enc: 1 (Low) → 3 (High)\")\n",
    "print(\"  - urgency_enc: 1 (Low) → 3 (High)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789b1aaa",
   "metadata": {},
   "source": [
    "#### 1.4.2 Label Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6b8aef",
   "metadata": {},
   "source": [
    "assignment_group and category have many unique values without natural ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "729a5257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Label encoded 2 variables:\n",
      "  - assignment_group_enc: 65 unique groups\n",
      "  - category_enc: 47 unique categories\n"
     ]
    }
   ],
   "source": [
    "# assignment_group (from EDA: 56% performance variance across groups)\n",
    "le_group = LabelEncoder()\n",
    "df_enc['assignment_group'] = df_enc['assignment_group'].fillna('Unknown')\n",
    "df_enc['assignment_group_enc'] = le_group.fit_transform(\n",
    "    df_enc['assignment_group'])\n",
    "\n",
    "# category (from EDA: 21.1% performance gap)\n",
    "le_cat = LabelEncoder()\n",
    "df_enc['category'] = df_enc['category'].fillna('Unknown')\n",
    "df_enc['category_enc'] = le_cat.fit_transform(df_enc['category'])\n",
    "\n",
    "print(f\"✓ Label encoded 2 variables:\")\n",
    "print(f\"  - assignment_group_enc: {len(le_group.classes_)} unique groups\")\n",
    "print(f\"  - category_enc: {len(le_cat.classes_)} unique categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb5c2e4",
   "metadata": {},
   "source": [
    "#### 1.4.3 Binary Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748df727",
   "metadata": {},
   "source": [
    "Binary features created during feature engineering need to be explicitly typed as integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "85788d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Verified 13 binary variables as integer type\n"
     ]
    }
   ],
   "source": [
    "binary_vars = [\n",
    "    'knowledge', 'u_priority_confirmation', 'made_sla', 'has_reassignment',\n",
    "    'high_modification', 'is_complex', 'is_high_priority', 'is_high_impact',\n",
    "    'is_high_urgency', 'is_high_severity', 'severity_complexity',\n",
    "    'confirmed_complex', 'priority_reassign'\n",
    "]\n",
    "\n",
    "for var in binary_vars:\n",
    "    if var in df_enc.columns:\n",
    "        df_enc[var] = df_enc[var].astype(int)\n",
    "\n",
    "verified_count = len([v for v in binary_vars if v in df_enc.columns])\n",
    "print(f\"✓ Verified {verified_count} binary variables as integer type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d703f217",
   "metadata": {},
   "source": [
    "### 2. Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9379032",
   "metadata": {},
   "source": [
    "Feature selection aims to identify the most predictive features while reducing dimensionality and preventing overfitting. We employ two complementary methods:\n",
    "1. Filter Method (ANOVA F-test):\n",
    "   - Statistical test measuring relationship between each feature and target\n",
    "   - Fast, model-agnostic approach\n",
    "   - Identifies univariate relationships\n",
    "2. Embedded Method (Random Forest Feature Importance):\n",
    "   - Features selected during model training\n",
    "   - Captures feature interactions and non-linear relationships\n",
    "   - Specific to tree-based models\n",
    "We combine both methods using the union of selected features to ensure\n",
    "we capture both statistical significance and model-specific importance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bd7634",
   "metadata": {},
   "source": [
    "### 2.1 Define Candidate Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bdd38d",
   "metadata": {},
   "source": [
    "Prepare feature groups based on their types and encoding methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "37d96224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate Features Summary:\n",
      "  Total: 20 features\n",
      "    - Numerical (including ordinal): 6\n",
      "    - Categorical (label encoded): 2\n",
      "    - Binary (flags): 12\n",
      "\n",
      "X shape: (6729, 20), y shape: (6729,)\n"
     ]
    }
   ],
   "source": [
    "# Define target variable\n",
    "target = 'made_sla'\n",
    "\n",
    "# Numerical features (continuous and ordinal encoded)\n",
    "numerical_features = [\n",
    "    'priority_enc',\n",
    "    'impact_enc',\n",
    "    'urgency_enc',  # Ordinal encoded severity\n",
    "    'reassignment_count',\n",
    "    'sys_mod_count',  # Raw operational counts\n",
    "    'activity_score'  # Weighted complexity\n",
    "]\n",
    "\n",
    "# Categorical features (label encoded)\n",
    "categorical_features = [\n",
    "    'assignment_group_enc',  # Support team assignment\n",
    "    'category_enc'  # Incident category\n",
    "]\n",
    "\n",
    "# Binary features (0/1 flags)\n",
    "binary_features = [\n",
    "    'knowledge',\n",
    "    'u_priority_confirmation',  # Process flags\n",
    "    'has_reassignment',\n",
    "    'high_modification',\n",
    "    'is_complex',  # Complexity flags\n",
    "    'is_high_priority',\n",
    "    'is_high_impact',\n",
    "    'is_high_urgency',\n",
    "    'is_high_severity',  # Severity flags\n",
    "    'severity_complexity',\n",
    "    'confirmed_complex',\n",
    "    'priority_reassign'  # Interaction flags\n",
    "]\n",
    "\n",
    "# Combine all candidate features\n",
    "all_features = numerical_features + categorical_features + binary_features\n",
    "available_features = [f for f in all_features if f in df_enc.columns]\n",
    "\n",
    "print(f\"Candidate Features Summary:\")\n",
    "print(f\"  Total: {len(available_features)} features\")\n",
    "print(\n",
    "    f\"    - Numerical (including ordinal): {len([f for f in numerical_features if f in df_enc.columns])}\"\n",
    ")\n",
    "print(\n",
    "    f\"    - Categorical (label encoded): {len([f for f in categorical_features if f in df_enc.columns])}\"\n",
    ")\n",
    "print(\n",
    "    f\"    - Binary (flags): {len([f for f in binary_features if f in df_enc.columns])}\"\n",
    ")\n",
    "\n",
    "X = df_enc[available_features].copy()\n",
    "y = df_enc[target].copy()\n",
    "X = X.fillna(X.median())\n",
    "\n",
    "print(f\"\\nX shape: {X.shape}, y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bd9d25",
   "metadata": {},
   "source": [
    "### 2.2 Prepare Feature Matrix and Target Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8628622",
   "metadata": {},
   "source": [
    "Extract features and target, handle missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "91df3a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix (X) shape: (6729, 20)\n",
      "Target vector (y) shape: (6729,)\n",
      "\n",
      "Target distribution:\n",
      "  Class 1 (SLA Met): 5,813 (86.4%)\n",
      "  Class 0 (SLA Breach): 916 (13.6%)\n",
      "  Class imbalance ratio: 6.35:1\n"
     ]
    }
   ],
   "source": [
    "# Prepare X (features) and y (target)\n",
    "X = df_enc[available_features].copy()\n",
    "y = df_enc[target].copy()\n",
    "\n",
    "# Handle missing values using median imputation\n",
    "# Median is robust to outliers and appropriate for numerical/ordinal features\n",
    "X = X.fillna(X.median())\n",
    "\n",
    "print(f\"Feature matrix (X) shape: {X.shape}\")\n",
    "print(f\"Target vector (y) shape: {y.shape}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(f\"  Class 1 (SLA Met): {y.sum():,} ({y.mean():.1%})\")\n",
    "print(\n",
    "    f\"  Class 0 (SLA Breach): {(~y.astype(bool)).sum():,} ({(1-y.mean()):.1%})\"\n",
    ")\n",
    "print(f\"  Class imbalance ratio: {y.sum() / (~y.astype(bool)).sum():.2f}:1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddcda08",
   "metadata": {},
   "source": [
    "### 2.3 Filter Method: ANOVA F-test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acf0155",
   "metadata": {},
   "source": [
    "This code selects the k best features from X that have the strongest statistical relationship with y according to the ANOVA F-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c8c178bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ANOVA Top 20:\n",
      "  1. confirmed_complex              F=1694.81\n",
      "  2. is_complex                     F=1446.74\n",
      "  3. activity_score                 F=1157.19\n",
      "  4. high_modification              F=1095.61\n",
      "  5. sys_mod_count                  F=1084.96\n",
      "  6. reassignment_count             F=463.56\n",
      "  7. has_reassignment               F=320.08\n",
      "  8. is_high_priority               F=192.76\n",
      "  9. is_high_severity               F=192.76\n",
      "  10. u_priority_confirmation        F=190.19\n"
     ]
    }
   ],
   "source": [
    "k_best = 20\n",
    "selector_anova = SelectKBest(score_func=f_classif, k=k_best)\n",
    "X_anova = selector_anova.fit_transform(X, y)\n",
    "\n",
    "anova_mask = selector_anova.get_support()\n",
    "anova_features = X.columns[anova_mask].tolist()\n",
    "anova_scores = selector_anova.scores_[anova_mask]\n",
    "\n",
    "print(f\"\\nANOVA Top {k_best}:\")\n",
    "for rank, (feat, score) in enumerate(\n",
    "        sorted(zip(anova_features, anova_scores),\n",
    "               key=lambda x: x[1],\n",
    "               reverse=True)[:10], 1):\n",
    "    print(f\"  {rank}. {feat:30s} F={score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc01b61f",
   "metadata": {},
   "source": [
    "### 2.4 Random Forest Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3266ad39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest Top 20:\n",
      "  1. sys_mod_count                  Imp=0.2025\n",
      "  2. activity_score                 Imp=0.1892\n",
      "  3. confirmed_complex              Imp=0.1610\n",
      "  4. is_complex                     Imp=0.1087\n",
      "  5. high_modification              Imp=0.1003\n",
      "  6. assignment_group_enc           Imp=0.0511\n",
      "  7. category_enc                   Imp=0.0471\n",
      "  8. reassignment_count             Imp=0.0258\n",
      "  9. u_priority_confirmation        Imp=0.0238\n",
      "  10. has_reassignment               Imp=0.0164\n",
      "\n",
      "Final: 20 features selected\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100,\n",
    "                            max_depth=10,\n",
    "                            random_state=42,\n",
    "                            n_jobs=-1,\n",
    "                            class_weight='balanced')\n",
    "rf.fit(X, y)\n",
    "\n",
    "importances = pd.Series(rf.feature_importances_,\n",
    "                        index=X.columns).sort_values(ascending=False)\n",
    "rf_features = importances.head(k_best).index.tolist()\n",
    "\n",
    "print(f\"\\nRandom Forest Top {k_best}:\")\n",
    "for rank, (feat, imp) in enumerate(importances.head(10).items(), 1):\n",
    "    print(f\"  {rank}. {feat:30s} Imp={imp:.4f}\")\n",
    "\n",
    "selected_features = list(set(anova_features + rf_features))\n",
    "selected_features.sort()\n",
    "\n",
    "print(f\"\\nFinal: {len(selected_features)} features selected\")\n",
    "\n",
    "X_selected = X[selected_features].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be13c955",
   "metadata": {},
   "source": [
    "### 2.5 Combine Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dbc2d421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "  ANOVA: 20 features\n",
      "  Random Forest: 20 features\n",
      "  Union: 20 features\n",
      "  Overlap: 20 features\n",
      "\n",
      "Final Selected Features (20):\n",
      "\n",
      "#    Feature                             ANOVA    RF      \n",
      "---------------------------------------------------------\n",
      "1    activity_score                      ✓        ✓       \n",
      "2    assignment_group_enc                ✓        ✓       \n",
      "3    category_enc                        ✓        ✓       \n",
      "4    confirmed_complex                   ✓        ✓       \n",
      "5    has_reassignment                    ✓        ✓       \n",
      "6    high_modification                   ✓        ✓       \n",
      "7    impact_enc                          ✓        ✓       \n",
      "8    is_complex                          ✓        ✓       \n",
      "9    is_high_impact                      ✓        ✓       \n",
      "10   is_high_priority                    ✓        ✓       \n",
      "11   is_high_severity                    ✓        ✓       \n",
      "12   is_high_urgency                     ✓        ✓       \n",
      "13   knowledge                           ✓        ✓       \n",
      "14   priority_enc                        ✓        ✓       \n",
      "15   priority_reassign                   ✓        ✓       \n",
      "16   reassignment_count                  ✓        ✓       \n",
      "17   severity_complexity                 ✓        ✓       \n",
      "18   sys_mod_count                       ✓        ✓       \n",
      "19   u_priority_confirmation             ✓        ✓       \n",
      "20   urgency_enc                         ✓        ✓       \n",
      "\n",
      "Dimensionality Reduction:\n",
      "  Original: 20 → Selected: 20 (0.0% reduction)\n"
     ]
    }
   ],
   "source": [
    "selected_features = list(set(anova_features + rf_features))\n",
    "selected_features.sort()\n",
    "\n",
    "print(f\"Results:\")\n",
    "print(f\"  ANOVA: {len(anova_features)} features\")\n",
    "print(f\"  Random Forest: {len(rf_features)} features\")\n",
    "print(f\"  Union: {len(selected_features)} features\")\n",
    "print(f\"  Overlap: {len(set(anova_features) & set(rf_features))} features\")\n",
    "\n",
    "print(f\"\\nFinal Selected Features ({len(selected_features)}):\\n\")\n",
    "print(f\"{'#':<4} {'Feature':<35} {'ANOVA':<8} {'RF':<8}\")\n",
    "print(\"-\" * 57)\n",
    "for i, feat in enumerate(selected_features, 1):\n",
    "    in_anova = \"✓\" if feat in anova_features else \" \"\n",
    "    in_rf = \"✓\" if feat in rf_features else \" \"\n",
    "    print(f\"{i:<4} {feat:<35} {in_anova:<8} {in_rf:<8}\")\n",
    "\n",
    "X_selected = X[selected_features].copy()\n",
    "\n",
    "print(f\"\\nDimensionality Reduction:\")\n",
    "print(\n",
    "    f\"  Original: {X.shape[1]} → Selected: {X_selected.shape[1]} ({(1-X_selected.shape[1]/X.shape[1])*100:.1f}% reduction)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920eff75",
   "metadata": {},
   "source": [
    "### 3. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dfcc16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Scaled 20 features\n",
      "\n",
      "✓ SMOTE: 6,729 → 11,626 samples\n",
      "\n",
      "✓ Saved to 'incidents_smote_balanced.csv'\n",
      "  Shape: (11626, 21)\n",
      "  Features: Pre-incident only (no post-incident leakage)\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_selected)\n",
    "X_scaled_df = pd.DataFrame(X_scaled,\n",
    "                           columns=X_selected.columns,\n",
    "                           index=X_selected.index)\n",
    "\n",
    "print(f\"\\n✓ Scaled {X_scaled_df.shape[1]} features\")\n",
    "\n",
    "## 4. SMOTE\n",
    "smote = SMOTE(random_state=42, k_neighbors=5)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_scaled_df, y)\n",
    "\n",
    "print(f\"\\n✓ SMOTE: {len(y):,} → {len(y_resampled):,} samples\")\n",
    "\n",
    "## 5. Save\n",
    "balanced_data = pd.DataFrame(X_resampled, columns=X_scaled_df.columns)\n",
    "balanced_data['made_sla'] = y_resampled\n",
    "balanced_data.to_csv('../data/incidents_smote_balanced.csv', index=False)\n",
    "\n",
    "print(f\"\\n✓ Saved to 'incidents_smote_balanced.csv'\")\n",
    "print(f\"  Shape: {balanced_data.shape}\")\n",
    "print(f\"  Features: Pre-incident only (no post-incident leakage)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp647_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
